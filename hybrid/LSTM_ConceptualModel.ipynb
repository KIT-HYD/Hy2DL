{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook to create a Hybrid Hydrological Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Description**\n",
    "\n",
    "The following notebook contains the code to create, train, validate and test a Hybrid Hydrological Model \n",
    "using a LSTM network plus a process based rainfall-runoff model. The code allows for the creation of single-basin \n",
    "models, but it is conceptualized to create regional models. The code is intended as an intial introduction to the topic, \n",
    "in which we prioritized interpretability over modularity.\n",
    "\n",
    "The logic of the code is heavily based on [Neural Hydrology](https://doi.org/10.21105/joss.04050)[1]. For a more \n",
    "flexible, robust and modular implementation of deep learning method in hydrological modeling we advice the use of Neural \n",
    "Hydrology. \n",
    "\n",
    "**Authors:**\n",
    "- Eduardo Acuna Espinoza (eduardo.espinoza@kit.edu)\n",
    "- Ralf Loritz\n",
    "- Manuel Álvarez Chaves\n",
    "\n",
    "**References:**\n",
    "\n",
    "[1]: \"F. Kratzert, M. Gauch, G. Nearing and D. Klotz: NeuralHydrology -- A Python library for Deep Learning research in hydrology. Journal of Open Source Software, 7, 4050, doi: 10.21105/joss.04050, 2022\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import necessary packages\n",
    "import sys\n",
    "import time\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from typing import List, Dict\n",
    "\n",
    "sys.path.append(\"../aux_functions\")\n",
    "from hydrological_models import SHM as conceptual_model  # define the conceptual model!\n",
    "from functions_datasets import CAMELS_GB as DataBase # define what you import as DataBase!\n",
    "from functions_training import nse_basin_averaged\n",
    "from functions_evaluation import nse\n",
    "from functions_aux import create_folder, set_random_seed, write_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 1. Initialize information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# paths to access the information\n",
    "path_entities = '../data/basin_id/basins_camels_gb_60.txt'\n",
    "path_data = '../data/CAMELS_GB'\n",
    "\n",
    "# dynamic forcings and target\n",
    "dynamic_input = ['precipitation', 'peti', 'temperature']\n",
    "conceptual_input = ['precipitation', 'peti', 'temperature']\n",
    "target = ['discharge_spec']\n",
    "\n",
    "# static attributes that will be used\n",
    "static_input = ['area',\n",
    "                'elev_mean',\n",
    "                'dpsbar',\n",
    "                'sand_perc',\n",
    "                'silt_perc',\n",
    "                'clay_perc',\n",
    "                'porosity_hypres',\n",
    "                'conductivity_hypres',\n",
    "                'soil_depth_pelletier',\n",
    "                'dwood_perc',\n",
    "                'ewood_perc',\n",
    "                'crop_perc',\n",
    "                'urban_perc',\n",
    "                'reservoir_cap',\n",
    "                'p_mean',\n",
    "                'pet_mean',\n",
    "                'p_seasonality',\n",
    "                'frac_snow',\n",
    "                'high_prec_freq',\n",
    "                'low_prec_freq',\n",
    "                'high_prec_dur',\n",
    "                'low_prec_dur']\n",
    "\n",
    "# time periods\n",
    "training_period = ['1980-10-01','1997-12-31']\n",
    "validation_period = ['1975-10-01','1980-09-30']\n",
    "testing_period = ['1998-01-01','2008-12-31']\n",
    "\n",
    "network_params = {\n",
    "  \"input_size\": len(dynamic_input) + len(static_input),\n",
    "  \"no_of_layers\":1,  \n",
    "  \"seq_length\": 180,\n",
    "  \"warmup_period\": 365,\n",
    "  \"training_length\": 365,\n",
    "  \"hidden_size\": 64,\n",
    "  \"batch_size\":10,\n",
    "  \"no_of_epochs\": 5,             \n",
    "  \"drop_out\": 0.4, \n",
    "  \"learning_rate\": 0.005,\n",
    "  \"adapt_learning_rate_epoch\": 10,\n",
    "  \"adapt_gamma_learning_rate\": 0.5,\n",
    "  'set_forget_gate':3,\n",
    "  \"n_conceptual_models\":1,\n",
    "  \"parameter_type\": {'dd': 'dynamic', 'f_thr': 'dynamic', 'sumax': 'dynamic', 'beta': 'dynamic', 'perc': 'dynamic',\n",
    "                     'kf': 'dynamic', 'ki': 'dynamic', 'kb': 'dynamic'}\n",
    "                     }\n",
    "\n",
    "# device\n",
    "running_device = 'cpu' #cpu or gpu\n",
    "\n",
    "# define seed\n",
    "seed = 42\n",
    "\n",
    "# Name of the folder where the results will be stored (the folder must be created before running\n",
    "# the code)\n",
    "path_save_folder = '../results/models/LSTM_SHM'\n",
    "\n",
    "# colorblind friendly palette for plotting\n",
    "color_palette = {'observed': '#1f78b4','LSTM': '#ff7f00'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create folder to store the results\n",
    "create_folder(folder_path=path_save_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 2. Class to create the dataset object used to manage the information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseDataset_Hybrid(Dataset):\n",
    "    \"\"\"Base data set class to load and preprocess data.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dynamic_input : List[str]\n",
    "        name of variables used as dynamic series input in the lstm\n",
    "    static_input : List[str]\n",
    "        name of static inputs used as input in the lstm (e.g. catchment attributes)\n",
    "    conceptual_input: List[str]\n",
    "        name of variables used as dynamic series input in the conceptual model\n",
    "    target: List[str]\n",
    "        target variable(s)\n",
    "    sequence_length: int\n",
    "        sequence length used in the LSTM\n",
    "    warmup_period: int\n",
    "        number of time intervals before the output of the LSTM is used in the conceptual model\n",
    "    time_period: List[str]\n",
    "        initial and final date (e.g. ['1987-10-01','1999-09-30']) of the time period of interest \n",
    "    path_entities: str\n",
    "        path to a txt file that contain the id of the entities (e.g. catchment`s ids) that will be analyzed\n",
    "    path_data: str\n",
    "        path to the folder were the data is stored\n",
    "    path_addional features: str\n",
    "        Optional parameter. Allows the option to add any arbitrary data that is not included in the standard data sets.\n",
    "        Path to a pickle file (or list of paths for multiple files), containing a dictionary with each key corresponding \n",
    "        to one basin id and the value is a date-time indexed pandas DataFrame.      \n",
    "    forcing: List[str]\n",
    "        For CAMELS-US dataset we should specificy which forcing data will be used (e.g. daymet, maurer, ndlas, etc.)\n",
    "    check_Nan: bool\n",
    "        Boolean that indicate if one should check of NaN values while processing the data\n",
    "    \"\"\"\n",
    "\n",
    "    #Function to initialize the data\n",
    "    def __init__(self, \n",
    "                 dynamic_input: List[str],\n",
    "                 static_input: List[str],\n",
    "                 conceptual_input: List[str],\n",
    "                 target: List[str], \n",
    "                 sequence_length: int,\n",
    "                 warmup_period: int,\n",
    "                 time_period: List[str],\n",
    "                 path_entities: str,\n",
    "                 path_data: str,\n",
    "                 path_additional_features: str = '',\n",
    "                 forcings: List[str] = [],\n",
    "                 check_NaN:bool = True\n",
    "                 ):\n",
    "\n",
    "        # read and create variables\n",
    "        self.time_period = time_period # time period that is being considered\n",
    "        self.dynamic_input = dynamic_input  # dynamic forcings going as inputs of in the lstm\n",
    "        self.conceptual_input = conceptual_input \n",
    "        self.target = target  # target variable\n",
    "        self.sequence_length = sequence_length # sequence length\n",
    "        self.warmup_period = warmup_period # warmup period\n",
    "\n",
    "        entities_ids = np.loadtxt(path_entities, dtype='str').tolist() \n",
    "        # save the cathments as a list even if there is just one\n",
    "        self.entities_ids = [entities_ids] if isinstance(entities_ids, str) else entities_ids # catchments\n",
    "\n",
    "        self.sequence_data = {} # store information that will be used to run the lstm\n",
    "        self.df_ts = {} # store processed dataframes for all basins\n",
    "        self.scaler = {} # information to standardize the data \n",
    "        self.basin_std = {} # std of the target variable of each basin (can be used later in the loss function)\n",
    "        self.valid_entities= []\n",
    "\n",
    "        # process the attributes\n",
    "        self.static_input = static_input # static attributes going as inputs to the lstm\n",
    "        if static_input:\n",
    "            self.df_attributes = self._load_attributes(path_data)\n",
    "\n",
    "        # process additional features that will be included in the inputs (optional) ---\n",
    "        if path_additional_features:\n",
    "            self.additional_features = self._load_additional_features(path_additional_features)\n",
    "        \n",
    "        # This loop goes through all the catchments. For each catchment in creates an entry in the dictionary\n",
    "        # self.sequence_data, where we will store the information that will be sent to the lstm\n",
    "        for id in self.entities_ids:\n",
    "            # load time series for specific catchment id\n",
    "            df_ts = self._load_data(path_data=path_data, catch_id=id, forcings = forcings)\n",
    "            # add additional features (optional)\n",
    "            if path_additional_features:\n",
    "                df_ts = pd.concat([df_ts, self.additional_features[id]], axis=1)\n",
    "            \n",
    "            # Defines the start date considering the offset due to sequence length. We want that, if possible, the start\n",
    "            # date is the first date of prediction.\n",
    "            start_date = pd.to_datetime(self.time_period[0],format=\"%Y-%m-%d\")\n",
    "            end_date = pd.to_datetime(self.time_period[1],format=\"%Y-%m-%d\")\n",
    "            freq = pd.infer_freq(df_ts.index)\n",
    "            warmup_start_date = start_date - (self.sequence_length+self.warmup_period-1)*pd.tseries.frequencies.to_offset(freq)\n",
    "            \n",
    "            # filter dataframe for the period and variables of interest\n",
    "            keep_columns = self.dynamic_input + list(set(self.conceptual_input) - set(self.dynamic_input)) + self.target\n",
    "            df_ts = df_ts.loc[warmup_start_date:end_date, keep_columns]\n",
    "            \n",
    "            # store dataframe\n",
    "            self.df_ts[id] = df_ts\n",
    "            \n",
    "            # create dictionary entry for the basin\n",
    "            self.sequence_data[id] = {}\n",
    "\n",
    "            # store the information of the basin in a nested dictionary\n",
    "            self.sequence_data[id]['x_d'] = torch.tensor(df_ts.loc[:, self.dynamic_input].values, dtype=torch.float32)\n",
    "            self.sequence_data[id]['x_c'] = torch.tensor(df_ts.loc[:, self.conceptual_input].values, dtype=torch.float32)\n",
    "            self.sequence_data[id]['y'] = torch.tensor(df_ts.loc[:, self.target].values, dtype=torch.float32)\n",
    "            if self.static_input:\n",
    "                self.sequence_data[id]['x_s'] = torch.tensor(self.df_attributes.loc[id].values, dtype=torch.float32)\n",
    "                    \n",
    "    def __len__(self):\n",
    "        return len(self.valid_entities)\n",
    "    \n",
    "    def __getitem__(self, id):\n",
    "        \"\"\"This function is used by PyTorch's dataloader to extract the information\"\"\"\n",
    "        basin, i = self.valid_entities[id]\n",
    "\n",
    "        # tensor of inputs LSTM\n",
    "        x_lstm = self.sequence_data[basin]['x_d'][i, :]\n",
    "        if self.static_input:\n",
    "            x_s = self.sequence_data[basin]['x_s'].repeat(x_lstm.shape[0],1)\n",
    "            x_lstm = torch.cat([x_lstm, x_s], dim=1)\n",
    "        \n",
    "        # tensor of inputs conceptual\n",
    "        x_conceptual = self.sequence_data[basin]['x_c'][i[self.sequence_length-1:], :]\n",
    "        \n",
    "        # tensor of outputs\n",
    "        y_obs = self.sequence_data[basin]['y'][i[self.sequence_length-1+self.warmup_period:], :]\n",
    "\n",
    "        # optional also return the basin_std\n",
    "        if self.basin_std:\n",
    "            basin_std = self.basin_std[basin].repeat(y_obs.size(0)).unsqueeze(1)\n",
    "            return x_lstm, x_conceptual, y_obs, basin_std\n",
    "        else:\n",
    "            return x_lstm, x_conceptual, y_obs\n",
    "        \n",
    "\n",
    "    def _load_attributes(self, path_data: str) -> pd.DataFrame:\n",
    "        \"\"\"Call the specific function that reads the static attributes information.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        path_data : str\n",
    "            path to the folder were the data is stored\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        df_attributes: pd.DataFrame\n",
    "            Dataframe containing the attributes of interest for the catchments of interest\n",
    "        \"\"\"\n",
    "        df_attributes = DataBase.read_attributes(path_data=path_data)\n",
    "        df_attributes = df_attributes.loc[self.entities_ids, self.static_input]\n",
    "        return df_attributes\n",
    "\n",
    "    def _load_data(self, path_data: str, catch_id:str, forcings:List[str]) -> pd.DataFrame:\n",
    "        \"\"\"Call the specific function that reads a specific catchment timeseries into a dataframe.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        path_data : str\n",
    "            path to the folder were the data is stored.\n",
    "        catch_id : str\n",
    "            basin_id.\n",
    "        forcings : str\n",
    "            Can be e.g. 'daymet' or 'nldas', etc. Must match the folder names in the 'basin_mean_forcing' directory. \n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        df: pd.DataFrame\n",
    "            Dataframe with the catchments` timeseries\n",
    "        \"\"\"\n",
    "        df_ts = DataBase.read_data(path_data=path_data, catch_id=catch_id, forcings = forcings)\n",
    "        return df_ts\n",
    "\n",
    "    def _load_additional_features(self, path_additional_features: str) -> Dict[str, pd.DataFrame]:\n",
    "        \"\"\"Read pickle dictionary containing additional features.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        path_additional_features : str\n",
    "            Path to a pickle file (or list of paths for multiple files), containing a dictionary with each key \n",
    "            corresponding to one basin id and the value is a date-time indexed pandas DataFrame.   \n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        additional_features: Dict[str, pd.DataFrame]\n",
    "            Dictionary where each key is a basin and each value is a date-time indexed pandas DataFrame with the \n",
    "            additional features\n",
    "        \"\"\"\n",
    "        \n",
    "        with open(path_additional_features, \"rb\") as file:\n",
    "            additional_features = pickle.load(file)\n",
    "        return additional_features\n",
    "  \n",
    "    def calculate_basin_std(self):\n",
    "        \"\"\"Fill the self.basin_std dictionary with the standard deviation of the target variables for each basin\"\"\"\n",
    "        for id, data in self.sequence_data.items():\n",
    "            self.basin_std[id] = torch.tensor(np.nanstd(data['y'].numpy()), dtype=torch.float32)\n",
    "    \n",
    "    def calculate_global_statistics(self):\n",
    "        \"\"\"Fill the self.scalar dictionary \n",
    "        \n",
    "        The function calculates the global mean and standard deviation of the dynamic inputs, target variables and \n",
    "        static attributes, and store the in a dictionary. It will be used later to standardize used in the LSTM. This\n",
    "        function should be called only in training period. \n",
    "        \"\"\"\n",
    "        global_x = np.vstack([df.loc[:, self.dynamic_input].values for df in self.df_ts.values()])\n",
    "        self.scaler['x_d_mean'] = torch.tensor(np.nanmean(global_x, axis=0), dtype=torch.float32)\n",
    "        self.scaler['x_d_std'] = torch.tensor(np.nanstd(global_x, axis=0), dtype=torch.float32)\n",
    "        del global_x\n",
    "\n",
    "        if self.static_input:\n",
    "            self.scaler['x_s_mean'] = torch.tensor(self.df_attributes.mean().values, dtype= torch.float32)\n",
    "            self.scaler['x_s_std'] = torch.tensor(self.df_attributes.std().values, dtype= torch.float32)\n",
    "    \n",
    "    def standardize_data(self):\n",
    "        \"\"\"Standardize the data used in the LSTM. \"\"\"\n",
    "        for basin in self.sequence_data.values():\n",
    "            # Standardize input\n",
    "            basin['x_d'] = (basin['x_d'] - self.scaler['x_d_mean']) / self.scaler['x_d_std']\n",
    "            if self.static_input:\n",
    "                basin['x_s'] = (basin['x_s'] - self.scaler['x_s_mean']) / self.scaler['x_s_std']\n",
    "\n",
    "    def generate_valid_entitites(self, training_length:int=0):\n",
    "        \"\"\"Generate the basin-time period combinations used for running the model \n",
    "\n",
    "        The function generate the basin_id-time period combinations that are used to run the model. When we are in\n",
    "        training, we specify a training_length. In validation and testing this parameter can be omitted, and it will\n",
    "        use the whole period as a valid entity\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        training_length:int\n",
    "            Number of timesteps (e.g. days) used to calculate the loss in training\n",
    "        \"\"\"\n",
    "        for key, df in self.df_ts.items():\n",
    "            if training_length >0:\n",
    "                time_cuts = list(range(self.sequence_length -1 + self.warmup_period, \n",
    "                                       len(df) + 1 - training_length, \n",
    "                                       training_length))\n",
    "                # Create maps\n",
    "                for cut in time_cuts:\n",
    "                    self.valid_entities.append([key, list(range(cut - (self.sequence_length - 1 + self.warmup_period), \n",
    "                                                                cut  + training_length))])\n",
    "                    \n",
    "            else:\n",
    "                self.valid_entities.append([key, list(range(0, len(df)))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class to create the hybrid model. Here is where I combine the LSTM network with the hydrological model\n",
    "class Hybrid_Model(nn.Module):\n",
    "    def __init__(self, network_params):\n",
    "        super().__init__()\n",
    "        # General information for the model\n",
    "        self.seq_length = network_params['seq_length']\n",
    "\n",
    "        self.num_features = network_params['input_size']\n",
    "        self.hidden_units = network_params['hidden_size']\n",
    "        self.num_layers = network_params['no_of_layers']\n",
    "\n",
    "        # conceptual model\n",
    "        self.conceptual_model = torch.jit.script(conceptual_model(n_models=network_params['n_conceptual_models'],\n",
    "                                                                  parameter_type=network_params['parameter_type']))\n",
    "        # linear layer\n",
    "        self.linear = nn.Linear(in_features=network_params['hidden_size'], \n",
    "                                out_features=len(self.conceptual_model.parameter_ranges) * network_params['n_conceptual_models']) \n",
    "        # lstm\n",
    "        self.lstm = nn.LSTM(input_size = network_params['input_size'], \n",
    "                            hidden_size = network_params['hidden_size'], \n",
    "                            batch_first = True,\n",
    "                            num_layers = network_params['no_of_layers'])\n",
    "           \n",
    "    def forward(self, x_lstm, x_conceptual):\n",
    "        \n",
    "        # Initialize hidden state with zeros\n",
    "        batch_size = x_lstm.shape[0]\n",
    "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_units, requires_grad=True, dtype=torch.float32, device=x_lstm.device)\n",
    "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_units, requires_grad=True, dtype=torch.float32, device=x_lstm.device)\n",
    "        \n",
    "        # run LSTM \n",
    "        lstm_out, _ = self.lstm(x_lstm, (h0, c0))\n",
    "        \n",
    "        # map lstm outputs to the dimension of the conceptual model´s parameters\n",
    "        lstm_out = lstm_out[:,self.seq_length-1:,:] # sequence to sequence\n",
    "        lstm_out = self.linear(lstm_out)\n",
    "        \n",
    "        # get predictions\n",
    "        pred = self.conceptual_model(x_conceptual=x_conceptual, lstm_out=lstm_out)\n",
    "\n",
    "        return pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 3. Create the different datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset training\n",
    "training_dataset = BaseDataset_Hybrid(dynamic_input=dynamic_input,\n",
    "                                      static_input=static_input,\n",
    "                                      conceptual_input=conceptual_input,\n",
    "                                      target=target,\n",
    "                                      sequence_length=network_params['seq_length'],\n",
    "                                      warmup_period=network_params['warmup_period'],\n",
    "                                      time_period=training_period,\n",
    "                                      path_entities=path_entities,\n",
    "                                      path_data=path_data,\n",
    "                                      check_NaN=False)\n",
    "\n",
    "training_dataset.calculate_basin_std()\n",
    "training_dataset.calculate_global_statistics() # the global statistics are calculated in the training period!\n",
    "training_dataset.standardize_data()\n",
    "training_dataset.generate_valid_entitites(training_length=network_params[\"training_length\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset validation\n",
    "validation_dataset = BaseDataset_Hybrid(dynamic_input=dynamic_input,\n",
    "                                        static_input=static_input,\n",
    "                                        conceptual_input=conceptual_input,\n",
    "                                        target=target,\n",
    "                                        sequence_length=network_params['seq_length'],\n",
    "                                        warmup_period=network_params['warmup_period'],\n",
    "                                        time_period=validation_period,\n",
    "                                        path_entities=path_entities,\n",
    "                                        path_data=path_data,\n",
    "                                        check_NaN=False)\n",
    "\n",
    "validation_dataset.scaler = training_dataset.scaler # read the global statisctics calculated in the training period\n",
    "validation_dataset.standardize_data()\n",
    "validation_dataset.generate_valid_entitites()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 4. Create the different dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create DataLoader for training data.\n",
    "train_loader = DataLoader(training_dataset, \n",
    "                          batch_size=network_params['batch_size'],\n",
    "                          shuffle=True,\n",
    "                          drop_last = True)\n",
    "\n",
    "print('Batches in training: ', len(train_loader))\n",
    "x_lstm, x_conceptual, y_obs, basin_std = next(iter(train_loader))\n",
    "print(f'x_lstm: {x_lstm.shape} | x_conceptual: {x_conceptual.shape} | y_obs: {y_obs.shape} | basin_std: {basin_std.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataLoader for validation data.\n",
    "validation_loader = DataLoader(validation_dataset, \n",
    "                               batch_size=1,\n",
    "                               shuffle=False,\n",
    "                               drop_last = False)\n",
    "\n",
    "# see if the batches are loaded correctly\n",
    "print('Batches in validation: ', len(validation_loader))\n",
    "x_lstm, x_conceptual, y_obs = next(iter(validation_loader))\n",
    "print(f'x_lstm: {x_lstm.shape} | x_conceptual: {x_conceptual.shape} | y_obs: {y_obs.shape}')\n",
    "\n",
    "# create some lists with the valid basins and the valid entities per basin that will help later to organize the data\n",
    "valid_basins = [key for key, _ in validation_dataset.df_ts.items()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 5. Define LSTM class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if model will be run in gpu or cpu\n",
    "if running_device == 'gpu':\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "    device= f'cuda:0'\n",
    "elif running_device == 'cpu':\n",
    "    device = \"cpu\"\n",
    "\n",
    "# construct model\n",
    "set_random_seed(seed=seed)\n",
    "hybrid_model = Hybrid_Model(network_params).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 6. Train LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer: Adam, learning rate, weight decay\n",
    "optimizer = torch.optim.Adam(hybrid_model.parameters(),\n",
    "                             lr = network_params[\"learning_rate\"])\n",
    "    \n",
    "# define learning rate scheduler / multiple ways to adapt the learning rate\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer,\n",
    "                                            step_size = network_params[\"adapt_learning_rate_epoch\"],\n",
    "                                            gamma = network_params[\"adapt_gamma_learning_rate\"])\n",
    "\n",
    "# set forget gate to 3 to ensure that the model is capable to learn long term dependencies\n",
    "hybrid_model.lstm.bias_hh_l0.data[network_params['hidden_size']:2 * network_params['hidden_size']] = network_params[\"set_forget_gate\"]\n",
    "\n",
    "training_time = time.time()\n",
    "for epoch in range(network_params[\"no_of_epochs\"]):\n",
    "\n",
    "    epoch_start_time = time.time()\n",
    "    total_loss = 0\n",
    "    # Training ----------------------------------------------------------------------\n",
    "    hybrid_model.train()\n",
    "    for x_lstm, x_conceptual, y, basin_std in train_loader: \n",
    "        optimizer.zero_grad() # sets gradients of weigths and bias to zero\n",
    "        pred = hybrid_model(x_lstm= x_lstm.to(device), \n",
    "                            x_conceptual= x_conceptual.to(device)) # forward call\n",
    "        \n",
    "        loss = nse_basin_averaged(y_sim = pred['y_hat'][:,training_dataset.warmup_period:], \n",
    "                                  y_obs = y.to(device),\n",
    "                                  per_basin_target_std = basin_std.to(device)) \n",
    "\n",
    "        loss.backward() # backpropagates\n",
    "        torch.nn.utils.clip_grad_norm_(hybrid_model.parameters(), 1) #clip gradients\n",
    "        optimizer.step() #update weights\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        # remove from cuda\n",
    "        torch.cuda.empty_cache()\n",
    "        del x_lstm, x_conceptual, y, pred\n",
    "    \n",
    "    # Average loss training\n",
    "    average_loss_training = total_loss / len(train_loader)\n",
    "    \n",
    "   # validation ----------------------------------------------------------------------\n",
    "    hybrid_model.eval()\n",
    "    validation_results={}\n",
    "    with torch.no_grad():\n",
    "        for i, (x_lstm, x_conceptual, y) in enumerate(validation_loader):\n",
    "            pred = hybrid_model(x_lstm= x_lstm.to(device), x_conceptual= x_conceptual.to(device)) # forward call\n",
    "\n",
    "            # join results in a dataframe and store them in a dictionary (is easier to plot later)\n",
    "            df_ts = validation_dataset.df_ts[valid_basins[i]].iloc[validation_dataset.sequence_length+\n",
    "                                                                validation_dataset.warmup_period-1:]\n",
    "            df_new = pd.DataFrame(data={'y_obs': y.flatten().cpu().detach().numpy(), \n",
    "                                        'y_sim': pred['y_hat'][:,validation_dataset.warmup_period:].flatten().cpu().detach().numpy()}, index=df_ts.index)\n",
    "            df_ts = pd.concat([df_ts, df_new], axis=1)\n",
    "            df_ts = df_ts.filter(['y_obs', 'y_sim'])\n",
    "            validation_results[valid_basins[i]] = df_ts\n",
    "\n",
    "            # remove from cuda\n",
    "            del x_lstm, x_conceptual, y, pred\n",
    "            torch.cuda.empty_cache()       \n",
    "            \n",
    "        #average loss validation\n",
    "        loss_validation = nse(df_results=validation_results)\n",
    "         \n",
    "    # save model after every epoch\n",
    "    path_saved_model = path_save_folder+'/epoch_' + str(epoch+1)\n",
    "    torch.save(hybrid_model.state_dict(), path_saved_model)\n",
    "            \n",
    "    # print epoch report\n",
    "    epoch_training_time = time.time()-epoch_start_time\n",
    "    LR = optimizer.param_groups[0]['lr']\n",
    "    report = f'Epoch: {epoch+1:<2} | Loss training: {\"%.3f \"% (average_loss_training)} | NSE validation: {\"%.3f \"% (loss_validation)} | LR:{\"%.5f \"% (LR)} | Training time: {\"%.1f \"% (epoch_training_time)} s'\n",
    "    print(report)\n",
    "    # save epoch report in txt file\n",
    "    write_report(file_path=path_save_folder+'/run_progress.txt', text=report)\n",
    "    # modify learning rate\n",
    "    scheduler.step()\n",
    "\n",
    "# print total report\n",
    "total_training_time = time.time()-training_time\n",
    "report = f'Total training time: {\"%.1f \"% (total_training_time)} s'\n",
    "print(report)\n",
    "# save total report in txt file\n",
    "write_report(file_path=path_save_folder+'/run_progress.txt', text=report) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 7. Test LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In case I already trained an LSTM I can re-construct the model\n",
    "#hybrid_model.load_state_dict(torch.load(path_save_folder + '/epoch_20'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset testing\n",
    "test_dataset = BaseDataset_Hybrid(dynamic_input=dynamic_input,\n",
    "                                  static_input=static_input,\n",
    "                                  conceptual_input=conceptual_input,\n",
    "                                  target=target,\n",
    "                                  sequence_length=network_params['seq_length'],\n",
    "                                  warmup_period=network_params['warmup_period'],\n",
    "                                  time_period=testing_period,\n",
    "                                  path_entities=path_entities,\n",
    "                                  path_data=path_data,\n",
    "                                  check_NaN=False)\n",
    "\n",
    "test_dataset.scaler = training_dataset.scaler # read the global statisctics calculated in the training period\n",
    "test_dataset.standardize_data()\n",
    "test_dataset.generate_valid_entitites()\n",
    "\n",
    "\n",
    "# DataLoader testing\n",
    "test_loader = DataLoader(test_dataset, \n",
    "                         batch_size=1,\n",
    "                         shuffle=False,\n",
    "                         drop_last = False)\n",
    "\n",
    "# see if the batches are loaded correctly\n",
    "print('Batches in testing: ', len(test_loader))\n",
    "x_lstm, x_conceptual, y_obs = next(iter(test_loader))\n",
    "print(f'x_lstm: {x_lstm.shape} | x_conceptual: {x_conceptual.shape} | y_obs: {y_obs.shape}')\n",
    "\n",
    "# create some lists with the valid basins and the valid entities per basin that will help later to organize the data\n",
    "valid_basins_testing = [key for key, _ in test_dataset.df_ts.items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing ----------------------------------------------------------------------\n",
    "hybrid_model.eval()\n",
    "test_results={}\n",
    "with torch.no_grad():\n",
    "    for i, (x_lstm, x_conceptual, y) in enumerate(test_loader):\n",
    "        \n",
    "        pred = hybrid_model(x_lstm= x_lstm.to(device), x_conceptual= x_conceptual.to(device)) # forward call\n",
    "\n",
    "        # join results in a dataframe and store them in a dictionary (is easier to plot later)\n",
    "        df_ts = test_dataset.df_ts[valid_basins_testing[i]].iloc[test_dataset.sequence_length+ test_dataset.warmup_period-1:]\n",
    "        \n",
    "        # extract the discharge information\n",
    "        df_discharge = pd.DataFrame(data={'y_obs': y.flatten().cpu().detach().numpy(), \n",
    "                                          'y_sim': pred['y_hat'][:, test_dataset.warmup_period:].flatten().cpu().detach().numpy()}, index=df_ts.index)\n",
    "        df_discharge = pd.concat([df_ts, df_discharge], axis=1)\n",
    "        df_discharge = df_discharge.filter(['y_obs', 'y_sim'])\n",
    "        \n",
    "        # extract internal_state (buckets) information\n",
    "        internal_states = {key: value[0, test_dataset.warmup_period:, :].cpu().detach().numpy() for key, value in pred['internal_states'].items()}\n",
    "        \n",
    "        # extract parameter  information\n",
    "        parameters = {key: value[0 , test_dataset.warmup_period:, :].cpu().detach().numpy() for key, value in pred['parameters'].items()}\n",
    "        \n",
    "        test_results[valid_basins_testing[i]] = {'discharges': df_discharge,\n",
    "                                                 'internal_states': internal_states,\n",
    "                                                 'parameters': parameters}\n",
    "\n",
    "        # remove from cuda\n",
    "        del x_lstm, x_conceptual, y, pred\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "# Save results as a pickle file\n",
    "#with open(path_save_folder+'/test_results.pickle', 'wb') as f:\n",
    " #   pickle.dump(test_results, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 8. Initial analysis of the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss testing\n",
    "discharges =  {key: value['discharges'] for key, value in test_results.items()}\n",
    "loss_testing = nse(df_results=discharges, average=False)\n",
    "df_NSE = pd.DataFrame(data={'basin_id': valid_basins_testing, 'NSE': np.round(loss_testing,3)})\n",
    "df_NSE = df_NSE.set_index('basin_id')\n",
    "#df_NSE.to_csv(path_save_folder+'/NSE.csv', index=True, header=True)\n",
    "\n",
    "# Save simulated and observed discharges for all the basins in csv format\n",
    "df_y_sim = pd.concat([pd.DataFrame({key: value['y_sim']}) for key, value in discharges.items()], axis=1)\n",
    "df_y_obs = pd.concat([pd.DataFrame({key: value['y_obs']}) for key, value in discharges.items()], axis=1)\n",
    "#df_y_sim.to_csv(path_save_folder+'/y_sim.csv', index=True, header=True)\n",
    "#df_y_obs.to_csv(path_save_folder+'/y_obs.csv', index=True, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Plot the histogram\n",
    "plt.hist(df_NSE['NSE'], bins=[0, .1, .2, .3, .4, .5, .6, .7, .8, .9, 1])\n",
    "\n",
    "# Add NSE statistics to the plot\n",
    "plt.text(0.01, 0.8, f'Mean: {\"%.2f\" % df_NSE[\"NSE\"].mean():>7}\\nMedian: {\"%.2f\" % df_NSE[\"NSE\"].median():>0}\\nMax: {\"%.2f\" % df_NSE[\"NSE\"].max():>9}\\nMin: {\"%.2f\" % df_NSE[\"NSE\"].min():>10}',\n",
    "         transform=plt.gca().transAxes, bbox=dict(facecolor='white', alpha=0.5))\n",
    "\n",
    "# Format plot\n",
    "plt.rcParams[\"figure.figsize\"] = (20, 5)\n",
    "plt.xlabel('NSE', fontsize=12, fontweight='bold')\n",
    "plt.ylabel('Frequency', fontsize=12, fontweight='bold')\n",
    "plt.title('NSE Histogram', fontsize=16, fontweight='bold')\n",
    "#plt.savefig(save_folder+'/NSE_Histogram.png', bbox_inches='tight', pad_inches=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot simulated and observed discharges\n",
    "basin_to_analyze = '2001'\n",
    "\n",
    "plt.plot(df_y_obs[basin_to_analyze], label=\"observed\", color=color_palette[\"observed\"])\n",
    "plt.plot(df_y_sim[basin_to_analyze], label=\"simulated\", alpha=0.5, color=color_palette[\"LSTM\"])\n",
    "\n",
    "# Format plot\n",
    "plt.xlabel('Day', fontsize=12, fontweight='bold')\n",
    "plt.ylabel('Discharge [mm/d]', fontsize=12, fontweight='bold')\n",
    "plt.title('Discharge comparison', fontsize=16, fontweight='bold')\n",
    "plt.tick_params(axis='both', which='major', labelsize=12)\n",
    "plt.legend(loc=\"upper right\",fontsize=12)\n",
    "#plt.savefig(save_folder+'/Model_Comparison.png', bbox_inches='tight', pad_inches=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot states\n",
    "state_of_interest = 'si'\n",
    "\n",
    "states = test_results[basin_to_analyze]['internal_states'][state_of_interest]\n",
    "\n",
    "for i in range(states.shape[1]):\n",
    "    plt.plot(df_y_obs.index, states[:, i], label=state_of_interest+'_'+str(i+1))\n",
    "\n",
    "# Adding labels and legend\n",
    "plt.xlabel('Day', fontsize=12, fontweight='bold')\n",
    "plt.ylabel('State [mm/d]', fontsize=12, fontweight='bold')\n",
    "plt.title('Time evolution of internal states of conceptual model', fontsize=16, fontweight='bold')\n",
    "plt.tick_params(axis='both', which='major', labelsize=12)\n",
    "plt.legend(loc=\"upper right\",fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot states\n",
    "parameter_of_interest= 'ki'\n",
    "\n",
    "states = test_results[basin_to_analyze]['parameters'][parameter_of_interest]\n",
    "\n",
    "for i in range(states.shape[1]):\n",
    "    plt.plot( df_y_obs.index, states[:, i], label=parameter_of_interest+'_'+str(i+1), alpha=0.8)\n",
    "\n",
    "# Adding labels and legend\n",
    "plt.xlabel('Day', fontsize=12, fontweight='bold')\n",
    "plt.ylabel('State [mm/d]', fontsize=12, fontweight='bold')\n",
    "plt.title('Time evolution of parameters of conceptual model', fontsize=16, fontweight='bold')\n",
    "plt.tick_params(axis='both', which='major', labelsize=12)\n",
    "plt.legend(loc=\"upper right\",fontsize=12)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "pytorch-gpu.2-0.m111",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.2-0:m111"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "42b7dc197ee81dd2f6541889b0e14556b882d218c1e7c97db94bc0f7b191f034"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
