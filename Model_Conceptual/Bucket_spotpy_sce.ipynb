{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notebook to calibrate a 'Bucket' hydrological model using the Shuffled Complex Evolution method. We use the spotpy library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Import necessary packages\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import spotpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code for the hydrological model\n",
    "def HydrologicalModel(X_SHM, initial_states, param):\n",
    "    #read initial states and parameters\n",
    "    si = initial_states\n",
    "    aux_ET, ki = param \n",
    "\n",
    "    # initialize vector to store discharges\n",
    "    q_out = np.zeros((X_SHM.shape[0], 1))\n",
    "\n",
    "    # run model for each timestep\n",
    "    for i, (p, pet, temp) in enumerate(X_SHM):\n",
    "        \n",
    "        # 1 bucket reservoir ------------------\n",
    "        si = si + p #[mm]\n",
    "        ret = pet * aux_ET #[mm]\n",
    "        si = max(0.0, si - ret) #[mm]\n",
    "        qi_out = si / ki #[mm]\n",
    "        si = si - qi_out #[mm]\n",
    "        # discharge\n",
    "        q_out[i,0] = qi_out #[mm]\n",
    "\n",
    "    return q_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class where I define the optimization object (following spotpy library examples)\n",
    "class spot_setup(object):\n",
    "    # optimization parameters\n",
    "    aux_ET = spotpy.parameter.Uniform(low=0.0, high=1.5)\n",
    "    ki = spotpy.parameter.Uniform(low=1.0, high=500.0)\n",
    "\n",
    "    def __init__(self, path_ts, basin_id, forcing, target, time_period, initial_conditions, buffer=0, obj_func=None):\n",
    "        # Read inputs ---------------------\n",
    "        self.basin_id = basin_id\n",
    "        self.buffer=buffer\n",
    "        self.initial_conditions = initial_conditions\n",
    "        self.obj_func = obj_func\n",
    "        \n",
    "        # load time series -----------------\n",
    "        path_timeseries = path_ts + 'CAMELS_GB_hydromet_timeseries_' + str(self.basin_id) + '.csv'\n",
    "        df_ts = pd.read_csv(path_timeseries)\n",
    "        \n",
    "        # forcings\n",
    "        df_forcing = df_ts.loc[:, forcing]\n",
    "        df_forcing = df_forcing.set_index('date')\n",
    "        # target\n",
    "        df_target = df_ts.loc[:, target]\n",
    "        df_target = df_target.set_index('date')\n",
    "        # training subset\n",
    "        df_forcing = df_forcing.loc[time_period[0]:time_period[1]]\n",
    "        df_target = df_target.loc[time_period[0]:time_period[1]]\n",
    "\n",
    "        self.X_SHM= df_forcing.to_numpy()\n",
    "        self.target = df_target.to_numpy().reshape((-1,1))\n",
    "        \n",
    "    def simulation(self, x):\n",
    "        sim_q = HydrologicalModel(self.X_SHM, self.initial_conditions, x)[:,0]\n",
    "        return sim_q\n",
    "    \n",
    "    def evaluation(self):\n",
    "        return self.target[:,0]\n",
    "    \n",
    "    def objectivefunction(self,simulation,evaluation, params=None):\n",
    "        if not self.obj_func: # if the user does not define a loss function\n",
    "            like = spotpy.objectivefunctions.rmse(evaluation[self.buffer:],simulation[self.buffer:])\n",
    "            # the self.buffer allow us to not consider the warmup period when we compute the loss\n",
    "        else:\n",
    "            like = self.obj_func(evaluation[self.buffer:],simulation[self.buffer:]) \n",
    "            # the self.buffer allow us to not consider the warmup period when we compute the loss  \n",
    "        return like\n",
    "    \n",
    "    def calibrated_values(self, q_sim, parameters):\n",
    "        self.q_sim = q_sim\n",
    "        self.calibrated_parameters = parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function used during optimization (NSE)\n",
    "def nse_loss(sim, obs):\n",
    "    nse_loss = np.sum((sim - obs)**2) / np.sum((obs - np.mean(obs))**2)\n",
    "    return np.round(1.0-nse_loss,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize information\n",
    "path_basins= '../data/CAMELS-GB/timeseries_v2/Selected_Basins_hybrid.csv'\n",
    "path_ts = '../data/CAMELS-GB/timeseries_v2/'\n",
    "path_output = '../results/models/Bucket/'\n",
    "buffer = 365 #warmup period\n",
    "initial_conditions = [5.0]\n",
    "forcing=['date','precipitation', 'peti', 'temperature']\n",
    "target=['date', 'discharge_spec']\n",
    "training_period = ['1987-10-01','1999-09-30']\n",
    "testing_period = ['2005-10-01','2012-09-30']\n",
    "# Read information\n",
    "selected_basins_id= list(np.loadtxt(path_basins, skiprows=1).astype(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folder '../results/models/Bucket/' already exists.\n"
     ]
    }
   ],
   "source": [
    "# Check if the path where one will store the results exists. In case it does not, it creates such path.\n",
    "if not os.path.exists(path_output):\n",
    "    # Create the folder\n",
    "    os.makedirs(path_output)\n",
    "    print(f\"Folder '{path_output}' created successfully.\")\n",
    "else:\n",
    "    print(f\"Folder '{path_output}' already exists.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create dataframe to store the results\n",
    "columns_name = ['basin_id', 'NSE_training', 'aux_ET', 'ki'] \n",
    "df_calibration = pd.DataFrame(index=range(len(selected_basins_id)), columns=columns_name)\n",
    "list_calibration = []\n",
    "\n",
    "# Loop to go through each basin that will be calibrated\n",
    "for i, basin in enumerate(selected_basins_id):\n",
    "    #Create setup object\n",
    "    list_calibration.append(spot_setup(path_ts= path_ts, \n",
    "                                       basin_id = basin,\n",
    "                                       forcing = forcing,\n",
    "                                       target = target,\n",
    "                                       time_period = training_period,\n",
    "                                       initial_conditions = initial_conditions, \n",
    "                                       buffer=buffer, \n",
    "                                       obj_func=None))\n",
    "    \n",
    "    file_name = path_output+'SCEUA_'+str(basin)\n",
    "    # Run calibration\n",
    "    sampler=spotpy.algorithms.sceua(list_calibration[i], dbname=file_name, dbformat='csv')\n",
    "    sampler.sample(10000, ngs=7, kstop=3, peps=0.1, pcento=0.1)\n",
    "    \n",
    "    #Get the results\n",
    "    results = spotpy.analyser.load_csv_results(file_name)\n",
    "    # Extract information about best run\n",
    "    bestindex,bestobjf = spotpy.analyser.get_minlikeindex(results)\n",
    "    best_model_run = results[bestindex]\n",
    "    # Extract calibrated parameters\n",
    "    par_fields=[word for word in best_model_run.dtype.names if word.startswith('par')]\n",
    "    parameters = list(best_model_run[par_fields])\n",
    "    # Calculate NSE of calibrated run\n",
    "    q_fields=[word for word in best_model_run.dtype.names if word.startswith('sim')]\n",
    "    q_sim = np.asarray(list(best_model_run[q_fields]))[buffer:]\n",
    "    NSE = nse_loss(sim=q_sim, obs=list_calibration[i].target[buffer:].flatten())\n",
    "    # Save the results\n",
    "    list_calibration[i].calibrated_values(q_sim, parameters)\n",
    "    row_data = [basin, NSE] + parameters\n",
    "    df_calibration.loc[i] = row_data\n",
    "    print('Calibration of basin:'+str(i+1)+'/'+str(len(selected_basins_id))+' with ID:'+str(basin)+' is completed-------------------------------')\n",
    "\n",
    "df_calibration.to_csv(path_output+'Bucket_SCE_calibration.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_calibration = pd.read_csv(path_output+'Bucket_SCE_calibration.csv')\n",
    "NSE_testing = []\n",
    "\n",
    "# Loop to go through each basin\n",
    "for i, basin in enumerate(selected_basins_id):\n",
    "     # read dataset for the basin of interest\n",
    "     path_timeseries = path_ts + 'CAMELS_GB_hydromet_timeseries_' + str(basin) + '.csv'\n",
    "     df_ts = pd.read_csv(path_timeseries)  \n",
    "     df_forcing = df_ts.loc[:, forcing]\n",
    "     df_forcing = df_forcing.set_index('date')\n",
    "     df_target = df_ts.loc[:, target]\n",
    "     df_target = df_target.set_index('date')\n",
    "\n",
    "     # Run model for testing period\n",
    "     df_forcing = df_forcing.loc[testing_period[0]:testing_period[1]]\n",
    "     df = df_calibration.loc[df_calibration['basin_id'] == basin]\n",
    "     param = np.ndarray.flatten(df.iloc[:, 2:].values).tolist()\n",
    "     q_sim = HydrologicalModel(df_forcing.to_numpy(), initial_conditions, param)\n",
    "     \n",
    "     # Observations for testing subset\n",
    "     df_target = df_target.loc[testing_period[0]:testing_period[1]]\n",
    "     q_obs = df_target.to_numpy().reshape((-1,1))\n",
    "     \n",
    "     # Calculate NSE in testing\n",
    "     NSE_testing.append(nse_loss(sim=q_sim[buffer:].flatten(), obs=q_obs[buffer:].flatten()))\n",
    "     print('Testing of basin:'+str(i+1)+'/'+str(len(selected_basins_id))+' with ID:'+str(basin)+' is completed-------------------------------')\n",
    "\n",
    "df_calibration['NSE_testing'] = NSE_testing\n",
    "df_calibration.to_csv(path_output+'Bucket_SCE_calibration.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "42b7dc197ee81dd2f6541889b0e14556b882d218c1e7c97db94bc0f7b191f034"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
