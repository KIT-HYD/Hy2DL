{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notebook to calibrate a hydrological model using the gradient descent method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.sampler import BatchSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class where I define my conceptual model\n",
    "class SHM(torch.nn.Module):\n",
    "    # Initialize the information\n",
    "    def __init__(self, parameter_ranges):\n",
    "        super().__init__()\n",
    "        # parameters that will be used for optimization\n",
    "        self.calibration_parameters = torch.nn.Parameter(torch.zeros(len(parameter_ranges),dtype=torch.float32))\n",
    "        # Define ranges for optimization\n",
    "        self.parameter_ranges = torch.tensor([i for i in parameter_ranges.values()], dtype=torch.float32)\n",
    "    \n",
    "    # call the function that runs the model\n",
    "    def forward(self, X_SHM, initial_states, warmup_period = 0):     \n",
    "        # run warmup period  (to stabilize the internal states of the model = buckets\n",
    "        if warmup_period>0:\n",
    "            with torch.no_grad():\n",
    "                _, states = self.run_SHM(X_SHM = X_SHM[0:warmup_period,:],\n",
    "                                         initial_states = initial_states)    \n",
    "        # calibration period\n",
    "        q_sim, _ = self.run_SHM(X_SHM = X_SHM[warmup_period:,:],\n",
    "                                initial_states = states)\n",
    "        # return simulated discharge\n",
    "        return q_sim\n",
    "        \n",
    "    # code for the hydrological model\n",
    "    def run_SHM(self, X_SHM, initial_states):\n",
    "        # map parameters between 0 and 1\n",
    "        sigmoid_params = torch.sigmoid(self.calibration_parameters)\n",
    "        # map parameters to calibration ranges\n",
    "        dd = self.parameter_ranges[0][0] + sigmoid_params[0]*(self.parameter_ranges[0][1]-self.parameter_ranges[0][0])\n",
    "        f_thr = self.parameter_ranges[1][0] + sigmoid_params[1]*(self.parameter_ranges[1][1]-self.parameter_ranges[1][0])\n",
    "        sumax = self.parameter_ranges[2][0] + sigmoid_params[2]*(self.parameter_ranges[2][1]-self.parameter_ranges[2][0])\n",
    "        beta = self.parameter_ranges[3][0] + sigmoid_params[3]*(self.parameter_ranges[3][1]-self.parameter_ranges[3][0])\n",
    "        perc = self.parameter_ranges[4][0] + sigmoid_params[4]*(self.parameter_ranges[4][1]-self.parameter_ranges[4][0])\n",
    "        kf = self.parameter_ranges[5][0] + sigmoid_params[5]*(self.parameter_ranges[5][1]-self.parameter_ranges[5][0])\n",
    "        ki = self.parameter_ranges[6][0] + sigmoid_params[6]*(self.parameter_ranges[6][1]-self.parameter_ranges[6][0])\n",
    "        kb = self.parameter_ranges[7][0] + sigmoid_params[7]*(self.parameter_ranges[7][1]-self.parameter_ranges[7][0])\n",
    "        \n",
    "        #read initial states and parameters\n",
    "        ss, sf, su, si, sb = initial_states\n",
    "        # initialize vector to store discharges\n",
    "        q_out = torch.zeros((X_SHM.shape[0], 1))\n",
    "        \n",
    "        # run model for each timestep\n",
    "        for j, (p, pet, temp) in enumerate(X_SHM):\n",
    "\n",
    "            # Snow module --------------------------\n",
    "            if temp > 0: # if there is snowmelt\n",
    "                qs_out = torch.min(ss, dd*temp) # snowmelt from snow reservoir\n",
    "                ss = ss - qs_out # substract snowmelt from snow reservoir\n",
    "                qsp_out = qs_out + p # flow from snowmelt and rainfall\n",
    "            else: # if the is no snowmelt\n",
    "                ss=ss + p # precipitation accumalates as snow in snow reservoir\n",
    "                qsp_out = torch.tensor(0.0, dtype=torch.float32) \n",
    "\n",
    "            # Split snowmelt+rainfall into inflow to fastflow reservoir and unsaturated reservoir ------\n",
    "            qf_in = torch.maximum(torch.tensor(0.0, dtype=torch.float32) , qsp_out-f_thr)\n",
    "            qu_in = torch.minimum(qsp_out, f_thr)\n",
    "\n",
    "            # Fastflow module ----------------------\n",
    "            sf = sf + qf_in\n",
    "            qf_out = sf/ kf\n",
    "            sf = sf - qf_out\n",
    "\n",
    "            # Unsaturated zone----------------------\n",
    "            psi = (su / sumax) ** beta #[-]\n",
    "            su_temp = su + qu_in * (1 - psi)\n",
    "            su = torch.minimum(su_temp, sumax)\n",
    "            qu_out = qu_in * psi + torch.maximum(torch.tensor(0.0, dtype=torch.float32), su_temp - sumax) # [mm]\n",
    "            \n",
    "            # Evapotranspiration\n",
    "            klu = torch.tensor(0.90, requires_grad=False, dtype=torch.float32) # land use correction factor [-]\n",
    "            if su == 0.0:\n",
    "                ktetha = torch.tensor(0.0, requires_grad=False, dtype=torch.float32)\n",
    "            elif su >= 0.8 * sumax:\n",
    "                ktetha = torch.tensor(1.0, requires_grad=False, dtype=torch.float32)\n",
    "            else:\n",
    "                ktetha = su / sumax\n",
    "\n",
    "            ret = pet * klu * ktetha #[mm]\n",
    "            su = torch.maximum(torch.tensor(0.0, requires_grad=True, dtype=torch.float32), su - ret) #[mm]\n",
    "\n",
    "            # Interflow reservoir ------------------\n",
    "            qi_in = qu_out * perc #[mm]\n",
    "            si = si + qi_in #[mm]\n",
    "            qi_out = si / ki #[mm]\n",
    "            si = si - qi_out #[mm]\n",
    "\n",
    "            # Baseflow reservoir -------------------\n",
    "            qb_in = qu_out * (1 - perc) #[mm]\n",
    "            sb = sb + qb_in #[mm]\n",
    "            qb_out = sb / kb #[mm]\n",
    "            sb = sb - qb_out #[mm]\n",
    "\n",
    "            # Output\n",
    "            states = torch.cat((ss.unsqueeze(0), sf.unsqueeze(0), su.unsqueeze(0), si.unsqueeze(0), sb.unsqueeze(0)))\n",
    "            q_out[j,0] = qf_out + qi_out + qb_out #[mm]\n",
    "\n",
    "        return q_out, states\n",
    "    \n",
    "    # return the calibrated parameters of the conceptual model\n",
    "    def calibrated_parameters(self):\n",
    "        # map parameters between 0 and 1\n",
    "        sigmoid_params = torch.sigmoid(self.calibration_parameters)\n",
    "        # map parameters to calibration ranges\n",
    "        dd = self.parameter_ranges[0][0] + sigmoid_params[0]*(self.parameter_ranges[0][1]-self.parameter_ranges[0][0])\n",
    "        f_thr = self.parameter_ranges[1][0] + sigmoid_params[1]*(self.parameter_ranges[1][1]-self.parameter_ranges[1][0])\n",
    "        sumax = self.parameter_ranges[2][0] + sigmoid_params[2]*(self.parameter_ranges[2][1]-self.parameter_ranges[2][0])\n",
    "        beta = self.parameter_ranges[3][0] + sigmoid_params[3]*(self.parameter_ranges[3][1]-self.parameter_ranges[3][0])\n",
    "        perc = self.parameter_ranges[4][0] + sigmoid_params[4]*(self.parameter_ranges[4][1]-self.parameter_ranges[4][0])\n",
    "        kf = self.parameter_ranges[5][0] + sigmoid_params[5]*(self.parameter_ranges[5][1]-self.parameter_ranges[5][0])\n",
    "        ki = self.parameter_ranges[6][0] + sigmoid_params[6]*(self.parameter_ranges[6][1]-self.parameter_ranges[6][0])\n",
    "        kb = self.parameter_ranges[7][0] + sigmoid_params[7]*(self.parameter_ranges[7][1]-self.parameter_ranges[7][0])\n",
    "\n",
    "        params = torch.cat((dd.unsqueeze(0), f_thr.unsqueeze(0), sumax.unsqueeze(0), \n",
    "                            beta.unsqueeze(0), perc.unsqueeze(0), kf.unsqueeze(0), \n",
    "                            ki.unsqueeze(0), kb.unsqueeze(0)))\n",
    "        \n",
    "        return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class where I define the dataset to calibrate the model (basically a big table where I organize\n",
    "# the inputs and targets used to calibrate the model)\n",
    "\n",
    "class Forcing_DataSet(Dataset):\n",
    "    def __init__(self,\n",
    "                 basin_id,  # ID of basin\n",
    "                 forcing,  # name of the dynamic forcings [list]\n",
    "                 target,  # name of the target [list]\n",
    "                 time_period,  # start and end day of time period of interest [list]\n",
    "                 path_ts  # path to time series [string]\n",
    "                 ):\n",
    "\n",
    "        # read variables and store them in self\n",
    "        self.time_period = time_period\n",
    "        self.basin_id = basin_id  # catchment ID\n",
    "        self.forcing = forcing  # dynamic attributes\n",
    "        self.target = target  # target\n",
    "\n",
    "        # full path_ts\n",
    "        path_timeseries = path_ts + 'CAMELS_GB_hydromet_timeseries_' + str(self.basin_id) + '.csv'\n",
    "\n",
    "        # load time series\n",
    "        df_ts = pd.read_csv(path_timeseries)\n",
    "\n",
    "        # load dynamic forcings\n",
    "        df_forcing = df_ts.loc[:, self.forcing]\n",
    "        df_forcing = df_forcing.set_index('date')\n",
    "\n",
    "        # load target value\n",
    "        df_target = df_ts.loc[:, self.target]\n",
    "        df_target = df_target.set_index('date')\n",
    "\n",
    "        # read training subset\n",
    "        df_forcing = df_forcing.loc[self.time_period[0]:self.time_period[1]]\n",
    "        df_target = df_target.loc[self.time_period[0]:self.time_period[1]]\n",
    "\n",
    "        # change all columns to float\n",
    "        self.df_forcing = df_forcing.astype(np.float64)\n",
    "        self.df_target = df_target.astype(np.float64)\n",
    "\n",
    "        # tensors with inputs and output\n",
    "        self.X = torch.tensor(self.df_forcing.values, dtype=torch.float32)\n",
    "        self.y = torch.tensor(self.df_target.values, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        # Function to define length of data (same as length of my time series)\n",
    "        return self.X.shape[0]\n",
    "\n",
    "    def __getitem__(self, id):\n",
    "        # Function to get the information during the optimization \n",
    "        return self.X[id, :], self.y[id]\n",
    "    \n",
    "    def year_batches(self, cutoff):\n",
    "        # Function to generate the year batches (one list for every year of data)\n",
    "        batches = []\n",
    "        minibatch = []\n",
    "        for id, date in enumerate(pd.DatetimeIndex(self.df_forcing.index)):\n",
    "            minibatch.append(id)\n",
    "            if date == datetime.datetime(date.year, cutoff[0], cutoff[1], cutoff[2]):\n",
    "                batches.append(minibatch)\n",
    "                minibatch = []\n",
    "                \n",
    "        return batches   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# During the optimization I will use batches of 2 years of data, one for warmup and the other for\n",
    "# calibration. This class is used so the DataLoader (pytorch class) can extract the information in\n",
    "# that way. \n",
    "\n",
    "class CustomSampler(BatchSampler):\n",
    "    def __init__(self, year_batches, suffle=True):\n",
    "        self.year_batches = year_batches\n",
    "        self.suffle=suffle\n",
    "\n",
    "    def __iter__(self):\n",
    "        # Groups two consequtive years, one for warmup period and the other for training\n",
    "        time_batches = [self.year_batches[i] + self.year_batches[i + 1] for i in range(len(self.year_batches) - 1)]\n",
    "        if self.suffle:\n",
    "            random.shuffle(time_batches)\n",
    "\n",
    "        # Deliver the combinations one by one to the DataLoader.\n",
    "        for batch in time_batches:\n",
    "            yield batch\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return (len(self.year_batches) - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# auxiliary function used by pytorch dataloader to return all the information tht is used during the\n",
    "# optimization process\n",
    "\n",
    "def collate_fn(batch):\n",
    "    return_list = []\n",
    "    for element in batch[0]:\n",
    "        return_list.append(element)\n",
    "    return return_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss used in optimization. Is the NSE without the initial one, because the optimization problem is\n",
    "# stated as a minimization\n",
    "def nse_loss(pred, obs):\n",
    "    nse_loss = torch.sum((pred - obs)**2) / torch.sum((obs - torch.mean(obs))**2)\n",
    "    return nse_loss\n",
    "\n",
    "# function used to train the model\n",
    "def train_model(data_loader, model, loss_function, optimizer, initial_states, warmup_period=0):\n",
    "    num_batches = len(data_loader)\n",
    "    total_loss = 0\n",
    "    \n",
    "    model.train()\n",
    "    # this for loop updates the optimization parameters for each batch. Each batch has two years of\n",
    "    # data, one for warmup and one for actual optimization\n",
    "    for X, y in data_loader:\n",
    "        optimizer.zero_grad()  \n",
    "        q_sim = model(X, initial_states, warmup_period) # run the model\n",
    "        loss = loss_function(q_sim, y[warmup_period:])  # calculate the loss\n",
    "        loss.backward()  # propagate the loss into the optimization parameters\n",
    "        optimizer.step() # optimize the parameters\n",
    "        total_loss += loss.item()\n",
    "                              \n",
    "    avg_loss = total_loss / num_batches # average loss during the whole epoch\n",
    "    \n",
    "    return q_sim, avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize information\n",
    "path_basins= '../data/CAMELS-GB/timeseries_v2/Selected_Basins_hybrid.csv'\n",
    "path_ts = '../data/CAMELS-GB/timeseries_v2/'\n",
    "path_output = '../results/models/SHM/'\n",
    "\n",
    "# dynamic forcings and target (ALWAYS INCLUDE THE DATE AS FIRST ARGUMENT)\n",
    "forcing=['date','precipitation', 'peti', 'temperature']\n",
    "target=['date', 'discharge_spec']\n",
    "\n",
    "# training period\n",
    "training_period = ['1987-10-01','1999-09-30']\n",
    "testing_period = ['2005-10-01','2012-09-30']\n",
    "warmup_period = 365\n",
    "\n",
    "# optimization hyperparameters\n",
    "optimization_hyperparameters = {\n",
    "    \"no_of_epochs\": 40,\n",
    "    \"learning_rate\": 0.05\n",
    "    }\n",
    "\n",
    "# optimization ranges\n",
    "parameter_ranges = {\n",
    "    'dd' : [0.0,10],\n",
    "    'f_thr'  : [10,60],\n",
    "    'sumax' : [20,700],\n",
    "    'beta'  : [1.0, 6.0],\n",
    "    'perc'  : [0.0, 1.0],\n",
    "    'kf'    : [1.0, 20.0],\n",
    "    'ki'    : [1.0, 100.0],\n",
    "    'kb'    : [10.0, 1000.0]\n",
    "}\n",
    "\n",
    "# initial states of the reservoirs (not too important because there is 1 year of warmup)\n",
    "initial_states = {\n",
    "    'ss_0'  : 0.0,\n",
    "    'sf_0'  : 1.0,\n",
    "    'su_0'  : 5.0,\n",
    "    'si_0'  : 10.0,\n",
    "    'sb_0'  : 15.0\n",
    "}\n",
    "\n",
    "# Read information from the basins that will be optimize\n",
    "selected_basins_id= list(np.loadtxt(path_basins, skiprows=1).astype(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the path where one will store the results exists. In case it does not, it creates such path.\n",
    "if not os.path.exists(path_output):\n",
    "    # Create the folder\n",
    "    os.makedirs(path_output)\n",
    "    print(f\"Folder '{path_output}' created successfully.\")\n",
    "else:\n",
    "    print(f\"Folder '{path_output}' already exists.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataframe to store the calibration results\n",
    "columns_name = ['basin_id', 'NSE_training', 'dd', 'f_thr', 'su_max', 'beta', 'perc', 'kf', 'ki', 'kb' , 'NSE_testing'] \n",
    "df_calibration = pd.DataFrame(index=range(len(selected_basins_id)), columns=columns_name)\n",
    "\n",
    "# Lists to store information during optimization\n",
    "SHM_models = []\n",
    "train_datasets = []\n",
    "test_datasets = []\n",
    "\n",
    "# Loop to go through each basin that will be calibrated\n",
    "for i, basin in enumerate(selected_basins_id):\n",
    "    # Dataset for training\n",
    "    train_datasets.append(Forcing_DataSet(basin_id=basin,\n",
    "                                          forcing=forcing,\n",
    "                                          target=target,\n",
    "                                          time_period=training_period,\n",
    "                                          path_ts=path_ts))\n",
    "\n",
    "    # DataLoader for training\n",
    "    year_batches = train_datasets[i].year_batches([9, 30, 0])\n",
    "    train_loader = DataLoader(train_datasets[i], \n",
    "                              sampler=CustomSampler(year_batches = year_batches),\n",
    "                              collate_fn=collate_fn)\n",
    "\n",
    "\n",
    "    # Define model\n",
    "    SHM_models.append(SHM(parameter_ranges))\n",
    "    states = torch.tensor([i for i in initial_states.values()], dtype=torch.float32)\n",
    "    # Define optimizer\n",
    "    optimizer = torch.optim.Adam(SHM_models[i].parameters(), lr=optimization_hyperparameters['learning_rate'])\n",
    "    \n",
    "    # Training --------------------------------------------------\n",
    "    for epoch in range(optimization_hyperparameters['no_of_epochs']): \n",
    "        q_sim, avg_loss= train_model(data_loader=train_loader, \n",
    "                                     model=SHM_models[i],\n",
    "                                     loss_function=nse_loss,\n",
    "                                     optimizer=optimizer, \n",
    "                                     initial_states=states,\n",
    "                                     warmup_period=warmup_period)\n",
    "        \n",
    "        print(f'Epoch: {epoch + 1:<2} | Loss: {\"%.4f \"% avg_loss}')\n",
    "\n",
    "    # Calculate NSE for whole training set (the loss during the previous loop was calculated by\n",
    "    # batches).\n",
    "    temp = [year_batches[0] , sum(year_batches[1:], [])]\n",
    "    loader_2 = DataLoader(train_datasets[i], sampler=CustomSampler(year_batches = temp),collate_fn=collate_fn)\n",
    "    X_training, y_training = next(iter(loader_2))\n",
    "    # run model\n",
    "    q_sim, _ = SHM_models[i].run_SHM(X_training, states)\n",
    "    #calculate training loss\n",
    "    nse_training = np.round(1.0 - nse_loss(q_sim[warmup_period:], y_training[warmup_period:]).detach().numpy(),3)\n",
    "    # get the optimized parameters\n",
    "    parameters = np.round(SHM_models[i].calibrated_parameters().detach().numpy(),3)\n",
    "\n",
    "    \n",
    "    # Testing --------------------------------------------------\n",
    "    test_datasets.append(Forcing_DataSet(basin_id=basin,\n",
    "                                         forcing=forcing,\n",
    "                                         target=target,\n",
    "                                         time_period=testing_period,\n",
    "                                         path_ts=path_ts))\n",
    "    \n",
    "    year_batches = test_datasets[i].year_batches([9, 30, 0])\n",
    "    temp = [year_batches[0] , sum(year_batches[1:], [])]\n",
    "    loader_2 = DataLoader(test_datasets[i], sampler=CustomSampler(year_batches = temp), collate_fn=collate_fn)\n",
    "    X_testing, y_testing = next(iter(loader_2))\n",
    "    # run model\n",
    "    q_sim, _ = SHM_models[i].run_SHM(X_testing, states)\n",
    "    # calculate testing loss\n",
    "    nse_testing = np.round(1.0 - nse_loss(q_sim[warmup_period:], y_testing[warmup_period:]).detach().numpy(),3)\n",
    "    \n",
    "    # Save the results\n",
    "    row_data = [basin, nse_training] + list(parameters) + [nse_testing]\n",
    "    df_calibration.loc[i] = row_data\n",
    "    print('Calibration of basin:'+str(i+1)+'/'+str(len(selected_basins_id))+' with ID:'+str(basin)+' is completed-------------------------------')\n",
    "\n",
    "# Save all the results in a dataframe\n",
    "df_calibration.to_csv(path_output+'SHM_SGD_calibration.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "35a50820829c02ecd58b03396d4cb9cc75e0372cdbeb523e83dfcac53b15f211"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
