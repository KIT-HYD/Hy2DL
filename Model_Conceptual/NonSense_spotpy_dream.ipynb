{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notebook to calibrate a 'NonSense' hydrological model using the DREAM method. We use the spotpy library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Import necessary packages\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import spotpy\n",
    "from spotpy.likelihoods import gaussianLikelihoodMeasErrorOut as GausianLike"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code for the hydrological model\n",
    "def HydrologicalModel(X_SHM, initial_states, param):\n",
    "\n",
    "    #read initial states and parameters\n",
    "    ss, su, si, sb = initial_states\n",
    "    dd, sumax, beta, ki, kb = param \n",
    "\n",
    "    # initialize vector to store discharges\n",
    "    q_out = np.zeros((X_SHM.shape[0], 1))\n",
    "\n",
    "    #run SHM model\n",
    "    for i, (p, pet, temp) in enumerate(X_SHM):\n",
    "        \n",
    "        # Snow module --------------------------\n",
    "        if temp > 0: # if there is snowmelt\n",
    "            qs_out = min(ss, dd*temp) # snowmelt from snow reservoir\n",
    "            ss = ss - qs_out # substract snowmelt from snow reservoir\n",
    "            qsp_out = qs_out + p # flow from snowmelt and rainfall\n",
    "        else: # if the is no snowmelt\n",
    "            ss=ss + p # precipitation accumalates as snow in snow reservoir\n",
    "            qsp_out = 0.0\n",
    "\n",
    "        # Baseflow reservoir -------------------\n",
    "        sb = sb + qsp_out #[mm]\n",
    "        qb_out = sb / kb #[mm]\n",
    "        sb = sb - qb_out #[mm]\n",
    "\n",
    "        # Interflow reservoir ------------------\n",
    "        si = si + qb_out #[mm]\n",
    "        qi_out = si / ki #[mm]\n",
    "        si = si - qi_out #[mm]\n",
    "        \n",
    "        # Unsaturated zone----------------------\n",
    "        psi = (su / sumax) ** beta #[-]\n",
    "        su_temp = su + qi_out * (1 - psi)\n",
    "        su = min(su_temp, sumax)\n",
    "        qu_out = qi_out * psi + max(0.0, su_temp - sumax) # [mm]\n",
    "\n",
    "        # Evapotranspiration -------------------\n",
    "        klu = 0.9 # land use correction factor [-]\n",
    "        if su <= 0.0:\n",
    "            ktetha = 0.0\n",
    "        elif su >= 0.8 * sumax:\n",
    "            ktetha = 1.0\n",
    "        else:\n",
    "            ktetha = su / sumax\n",
    "\n",
    "        ret = pet * klu * ktetha #[mm]\n",
    "        su = max(0.0, su - ret) #[mm]\n",
    "\n",
    "        # Output\n",
    "        q_out[i,0] =  qu_out #[mm]\n",
    "\n",
    "    return q_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class where I define the optimization object (following spotpy library examples)\n",
    "class spot_setup(object):\n",
    "    # optimization parameters\n",
    "    dd = spotpy.parameter.Uniform(low=0.0, high=10)\n",
    "    su_max = spotpy.parameter.Uniform(low=20.0, high=700.0)\n",
    "    beta = spotpy.parameter.Uniform(low=1.0, high=6.0)\n",
    "    ki = spotpy.parameter.Uniform(low=1.0, high=100.0)\n",
    "    kb = spotpy.parameter.Uniform(low=10.0, high=1000.0)\n",
    "\n",
    "    def __init__(self, path_ts, basin_id, forcing, target, time_period, initial_conditions, buffer=0, obj_func=None):\n",
    "        # Read inputs ---------------------\n",
    "        self.basin_id = basin_id\n",
    "        self.buffer=buffer\n",
    "        self.initial_conditions = initial_conditions\n",
    "        self.obj_func = obj_func\n",
    "        \n",
    "        # load time series -----------------\n",
    "        path_timeseries = path_ts + 'CAMELS_GB_hydromet_timeseries_' + str(self.basin_id) + '.csv'\n",
    "        df_ts = pd.read_csv(path_timeseries)\n",
    "        \n",
    "        # forcings\n",
    "        df_forcing = df_ts.loc[:, forcing]\n",
    "        df_forcing = df_forcing.set_index('date')\n",
    "        # target\n",
    "        df_target = df_ts.loc[:, target]\n",
    "        df_target = df_target.set_index('date')\n",
    "        # training subset\n",
    "        df_forcing = df_forcing.loc[time_period[0]:time_period[1]]\n",
    "        df_target = df_target.loc[time_period[0]:time_period[1]]\n",
    "\n",
    "        self.X_SHM= df_forcing.to_numpy()\n",
    "        self.target = df_target.to_numpy().reshape((-1,1))\n",
    "        \n",
    "    def simulation(self, x):\n",
    "        sim_q = HydrologicalModel(self.X_SHM, self.initial_conditions, x)[:,0]\n",
    "        return sim_q\n",
    "    \n",
    "    def evaluation(self):\n",
    "        return self.target[:,0]\n",
    "    \n",
    "    def objectivefunction(self,simulation,evaluation, params=None):\n",
    "        if not self.obj_func: #if the user does not define a loss function\n",
    "            like = spotpy.objectivefunctions.rmse(evaluation[self.buffer:],simulation[self.buffer:])\n",
    "            # the self.buffer allow us to not consider the warmup period when we compute the loss\n",
    "        else:\n",
    "            like = self.obj_func(evaluation[self.buffer:],simulation[self.buffer:])\n",
    "            # the self.buffer allow us to not consider the warmup period when we compute the loss \n",
    "        return like\n",
    "    \n",
    "    def calibrated_values(self, q_sim, parameters):\n",
    "        self.q_sim = q_sim\n",
    "        self.calibrated_parameters = parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function used during optimization (NSE)\n",
    "def nse_loss(sim, obs):\n",
    "    nse_loss = np.sum((sim - obs)**2) / np.sum((obs - np.mean(obs))**2)\n",
    "    return np.round(1.0-nse_loss,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize information\n",
    "path_basins= '../data/CAMELS-GB/timeseries_v2/Selected_Basins_hybrid.csv'\n",
    "path_ts = '../data/CAMELS-GB/timeseries_v2/'\n",
    "path_output = '../results/models/NonSense/'\n",
    "buffer = 365\n",
    "initial_conditions = [0.0, 5.0, 10.0, 15.0]\n",
    "forcing=['date','precipitation', 'peti', 'temperature']\n",
    "target=['date', 'discharge_spec']\n",
    "training_period = ['1987-10-01','1999-09-30']\n",
    "testing_period = ['2005-10-01','2012-09-30']\n",
    "# Read information\n",
    "selected_basins_id= list(np.loadtxt(path_basins, skiprows=1).astype(int))\n",
    "\n",
    "# Hyperparameters of calibration method\n",
    "parallel = \"seq\"\n",
    "delta = 3\n",
    "nChains = 7\n",
    "convergence_limit = 1.2\n",
    "c = 0.1\n",
    "nCr = 3\n",
    "eps = 10e-6\n",
    "runs_after_convergence = 100\n",
    "acceptance_test_option = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the path where one will store the results exists. In case it does not, it creates such path.\n",
    "if not os.path.exists(path_output):\n",
    "    # Create the folder\n",
    "    os.makedirs(path_output)\n",
    "    print(f\"Folder '{path_output}' created successfully.\")\n",
    "else:\n",
    "    print(f\"Folder '{path_output}' already exists.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create dataframe to store the results\n",
    "columns_name = ['basin_id', 'NSE_training', 'dd', 'su_max', 'beta', 'ki', 'kb'] \n",
    "df_calibration = pd.DataFrame(index=range(len(selected_basins_id)), columns=columns_name)\n",
    "list_calibration = []\n",
    "\n",
    "# Loop to go through each basin that will be calibrated\n",
    "for i, basin in enumerate(selected_basins_id):\n",
    "    #Create setup object\n",
    "    list_calibration.append(spot_setup(path_ts= path_ts, \n",
    "                                       basin_id = basin,\n",
    "                                       forcing = forcing,\n",
    "                                       target = target,\n",
    "                                       time_period = training_period,\n",
    "                                       initial_conditions = initial_conditions, \n",
    "                                       buffer=buffer, \n",
    "                                       obj_func=GausianLike))\n",
    "    \n",
    "    file_name = path_output+'DREAM_'+str(basin)\n",
    "    # Run calibration\n",
    "    sampler = spotpy.algorithms.dream(list_calibration[i], dbname=file_name, dbformat=\"csv\", parallel=parallel)\n",
    "    r_hat = sampler.sample(5000,nChains,nCr,delta,c,eps,convergence_limit,runs_after_convergence,acceptance_test_option)\n",
    "    \n",
    "    #Get the results\n",
    "    results = spotpy.analyser.load_csv_results(file_name)\n",
    "    # Extract information about best run\n",
    "    bestindex,bestobjf = spotpy.analyser.get_maxlikeindex(results)\n",
    "    best_model_run = results[bestindex[0][0]]\n",
    "    # Extract calibrated parameters\n",
    "    par_fields=[word for word in best_model_run.dtype.names if word.startswith('par')]\n",
    "    parameters = list(best_model_run[par_fields])\n",
    "    # Calculate NSE of calibrated run\n",
    "    q_fields=[word for word in best_model_run.dtype.names if word.startswith('sim')]\n",
    "    q_sim = np.asarray(list(best_model_run[q_fields]))[buffer:]\n",
    "    NSE = nse_loss(sim=q_sim, obs=list_calibration[i].target[buffer:].flatten())\n",
    "    # Save the results\n",
    "    list_calibration[i].calibrated_values(q_sim, parameters)\n",
    "    row_data = [basin, NSE] + parameters\n",
    "    df_calibration.loc[i] = row_data\n",
    "    print('Calibration of basin:'+str(i+1)+'/'+str(len(selected_basins_id))+' with ID:'+str(basin)+' is completed-------------------------------')\n",
    "\n",
    "df_calibration.to_csv(path_output+'NonSense_DREAM_calibration.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_calibration = pd.read_csv(path_output+'NonSense_DREAM_calibration.csv')\n",
    "NSE_testing = []\n",
    "\n",
    "# Loop to go through each basin\n",
    "for i, basin in enumerate(selected_basins_id):\n",
    "     # read dataset for the basin of interest\n",
    "     path_timeseries = path_ts + 'CAMELS_GB_hydromet_timeseries_' + str(basin) + '.csv'\n",
    "     df_ts = pd.read_csv(path_timeseries)  \n",
    "     df_forcing = df_ts.loc[:, forcing]\n",
    "     df_forcing = df_forcing.set_index('date')\n",
    "     df_target = df_ts.loc[:, target]\n",
    "     df_target = df_target.set_index('date')\n",
    "\n",
    "     # Run model for testing period\n",
    "     df_forcing = df_forcing.loc[testing_period[0]:testing_period[1]]\n",
    "     df = df_calibration.loc[df_calibration['basin_id'] == basin]\n",
    "     param = np.ndarray.flatten(df.iloc[:, 2:].values).tolist()\n",
    "     q_sim = HydrologicalModel(df_forcing.to_numpy(), initial_conditions, param)\n",
    "     \n",
    "     # Observations for testing subset\n",
    "     df_target = df_target.loc[testing_period[0]:testing_period[1]]\n",
    "     q_obs = df_target.to_numpy().reshape((-1,1))\n",
    "     \n",
    "     # Calculate NSE in testing\n",
    "     NSE_testing.append(nse_loss(sim=q_sim[buffer:].flatten(), obs=q_obs[buffer:].flatten()))\n",
    "     print('Testing of basin:'+str(i+1)+'/'+str(len(selected_basins_id))+' with ID:'+str(basin)+' is completed-------------------------------')\n",
    "\n",
    "df_calibration['NSE_testing'] = NSE_testing\n",
    "df_calibration.to_csv(path_output+'NonSense_DREAM_calibration.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "42b7dc197ee81dd2f6541889b0e14556b882d218c1e7c97db94bc0f7b191f034"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
