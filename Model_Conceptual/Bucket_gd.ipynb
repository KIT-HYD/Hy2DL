{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notebook to calibrate a 'Bucket' hydrological model using the gradient descent method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.sampler import BatchSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class where I define my conceptual model\n",
    "class SHM(torch.nn.Module):\n",
    "    # Initialize the information\n",
    "    def __init__(self, parameter_ranges):\n",
    "        super().__init__()\n",
    "        # parameters that will be used for optimization\n",
    "        self.calibration_parameters = torch.nn.Parameter(torch.zeros(len(parameter_ranges),dtype=torch.float32))\n",
    "        # Define ranges for optimization\n",
    "        self.parameter_ranges = torch.tensor([i for i in parameter_ranges.values()], dtype=torch.float32)\n",
    "    \n",
    "    # call the function that runs the model\n",
    "    def forward(self, X_SHM, initial_states, warmup_period = 0):     \n",
    "        # run warmup period  (to stabilize the internal states of the model = buckets)\n",
    "        if warmup_period>0:\n",
    "            with torch.no_grad():\n",
    "                _, states = self.run_SHM(X_SHM = X_SHM[0:warmup_period,:],\n",
    "                                         initial_states = initial_states)    \n",
    "        # calibration period\n",
    "        q_sim, _ = self.run_SHM(X_SHM = X_SHM[warmup_period:,:],\n",
    "                                initial_states = states)\n",
    "        # return simulated discharge\n",
    "        return q_sim\n",
    "        \n",
    "    # code for the hydrological model\n",
    "    def run_SHM(self, X_SHM, initial_states):\n",
    "        # map parameters between 0 and 1\n",
    "        sigmoid_params = torch.sigmoid(self.calibration_parameters)\n",
    "        # map parameters to calibration ranges\n",
    "        aux_ET = self.parameter_ranges[0][0] + sigmoid_params[0]*(self.parameter_ranges[0][1]-self.parameter_ranges[0][0])\n",
    "        ki = self.parameter_ranges[1][0] + sigmoid_params[1]*(self.parameter_ranges[1][1]-self.parameter_ranges[1][0])\n",
    "        \n",
    "        #read initial states\n",
    "        si = initial_states\n",
    "        # initialize vector to store discharges\n",
    "        q_out = torch.zeros((X_SHM.shape[0], 1))\n",
    "        \n",
    "        # run model for each timestep\n",
    "        for j, (p, pet, temp) in enumerate(X_SHM):\n",
    "\n",
    "            # 1 bucket reservoir ------------------\n",
    "            si = si + p #[mm]\n",
    "            ret = pet * aux_ET #[mm]\n",
    "            si = torch.maximum(torch.tensor(0.0, requires_grad=True, dtype=torch.float32), si - ret) #[mm]\n",
    "            qi_out = si / ki #[mm]\n",
    "            si = si - qi_out #[mm]\n",
    "\n",
    "            # discharge\n",
    "            q_out[j,0] = qi_out #[mm]   \n",
    "        \n",
    "        return q_out, si\n",
    "    \n",
    "    # return the calibrated parameters of the conceptual model\n",
    "    def calibrated_parameters(self):\n",
    "        # map parameters between 0 and 1\n",
    "        sigmoid_params = torch.sigmoid(self.calibration_parameters)\n",
    "        # map parameters to calibration ranges\n",
    "        aux_ET = self.parameter_ranges[0][0] + sigmoid_params[0]*(self.parameter_ranges[0][1]-self.parameter_ranges[0][0])\n",
    "        ki = self.parameter_ranges[1][0] + sigmoid_params[1]*(self.parameter_ranges[1][1]-self.parameter_ranges[1][0])\n",
    "\n",
    "        params = torch.cat((aux_ET.unsqueeze(0), ki.unsqueeze(0)))\n",
    "        \n",
    "        return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class where I define the dataset to calibrate the model (basically a big table where I organize\n",
    "# the inputs and targets used to calibrate the model)\n",
    "\n",
    "class Forcing_DataSet(Dataset):\n",
    "    def __init__(self,\n",
    "                 basin_id,  # ID of basin\n",
    "                 forcing,  # name of the dynamic forcings [list]\n",
    "                 target,  # name of the target [list]\n",
    "                 time_period,  # start and end day of time period of interest [list]\n",
    "                 path_ts  # path to time series [string]\n",
    "                 ):\n",
    "\n",
    "        # read variables and store them in self\n",
    "        self.time_period = time_period\n",
    "        self.basin_id = basin_id  # catchment ID\n",
    "        self.forcing = forcing  # dynamic attributes\n",
    "        self.target = target  # target\n",
    "\n",
    "        # full path_ts\n",
    "        path_timeseries = path_ts + 'CAMELS_GB_hydromet_timeseries_' + str(self.basin_id) + '.csv'\n",
    "\n",
    "        # load time series\n",
    "        df_ts = pd.read_csv(path_timeseries)\n",
    "\n",
    "        # load dynamic forcings\n",
    "        df_forcing = df_ts.loc[:, self.forcing]\n",
    "        df_forcing = df_forcing.set_index('date')\n",
    "\n",
    "        # load target value\n",
    "        df_target = df_ts.loc[:, self.target]\n",
    "        df_target = df_target.set_index('date')\n",
    "\n",
    "        # read training subset\n",
    "        df_forcing = df_forcing.loc[self.time_period[0]:self.time_period[1]]\n",
    "        df_target = df_target.loc[self.time_period[0]:self.time_period[1]]\n",
    "\n",
    "        # change all columns to float\n",
    "        self.df_forcing = df_forcing.astype(np.float64)\n",
    "        self.df_target = df_target.astype(np.float64)\n",
    "\n",
    "        # tensors with inputs and output\n",
    "        self.X = torch.tensor(self.df_forcing.values, dtype=torch.float32)\n",
    "        self.y = torch.tensor(self.df_target.values, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        # Function to define length of data (same as length of my time series)\n",
    "        return self.X.shape[0]\n",
    "\n",
    "    def __getitem__(self, id):\n",
    "        # Function to get the information during the optimization \n",
    "        return self.X[id, :], self.y[id]\n",
    "    \n",
    "    def year_batches(self, cutoff):\n",
    "        # Function to generate the year batches (one list for every year of data)\n",
    "        batches = []\n",
    "        minibatch = []\n",
    "        for id, date in enumerate(pd.DatetimeIndex(self.df_forcing.index)):\n",
    "            minibatch.append(id)\n",
    "            if date == datetime.datetime(date.year, cutoff[0], cutoff[1], cutoff[2]):\n",
    "                batches.append(minibatch)\n",
    "                minibatch = []\n",
    "                \n",
    "        return batches   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# During the optimization I will use batches of 2 years of data, one for warmup and the other for\n",
    "# calibration. This class is used so the DataLoader (pytorch class) can extract the information in\n",
    "# that way. \n",
    "\n",
    "class CustomSampler(BatchSampler):\n",
    "    def __init__(self, year_batches, suffle=True):\n",
    "        self.year_batches = year_batches\n",
    "        self.suffle=suffle\n",
    "\n",
    "    def __iter__(self):\n",
    "        # Groups two consequtive years, one for warmup period and the other for training\n",
    "        time_batches = [self.year_batches[i] + self.year_batches[i + 1] for i in range(len(self.year_batches) - 1)]\n",
    "        if self.suffle:\n",
    "            random.shuffle(time_batches)\n",
    "\n",
    "        # Deliver the combinations one by one to the DataLoader.\n",
    "        for batch in time_batches:\n",
    "            yield batch\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return (len(self.year_batches) - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# auxiliary function used by pytorch dataloader to return all the information tht is used during the\n",
    "# optimization process\n",
    "def collate_fn(batch):\n",
    "    return_list = []\n",
    "    for element in batch[0]:\n",
    "        return_list.append(element)\n",
    "    return return_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss used in optimization. Is the NSE without the initial one, because the optimization problem is\n",
    "# stated as a minimization\n",
    "def nse_loss(pred, obs):\n",
    "    nse_loss = torch.sum((pred - obs)**2) / torch.sum((obs - torch.mean(obs))**2)\n",
    "    return nse_loss\n",
    "\n",
    "# function used to train the model\n",
    "def train_model(data_loader, model, loss_function, optimizer, initial_states, warmup_period=0):\n",
    "    num_batches = len(data_loader)\n",
    "    total_loss = 0\n",
    "    \n",
    "    model.train()\n",
    "    # this for loop updates the optimization parameters for each batch. Each batch has two years of\n",
    "    # data, one for warmup and one for actual optimization\n",
    "    for X, y in data_loader:\n",
    "        optimizer.zero_grad()  \n",
    "        q_sim = model(X, initial_states, warmup_period) # run the model\n",
    "        loss = loss_function(q_sim, y[warmup_period:]) # calculate the loss\n",
    "        loss.backward() # propagate the loss into the optimization parameters\n",
    "        optimizer.step() # optimize the parameters \n",
    "        total_loss += loss.item()\n",
    "                              \n",
    "    avg_loss = total_loss / num_batches # average loss during the whole epoch\n",
    "    \n",
    "    return q_sim, avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize information\n",
    "path_basins= '../data/CAMELS-GB/timeseries_v2/Selected_Basins_hybrid.csv'\n",
    "path_ts = '../data/CAMELS-GB/timeseries_v2/'\n",
    "path_output = '../results/models/Bucket/'\n",
    "\n",
    "# dynamic forcings and target (ALWAYS INCLUDE THE DATE AS FIRST ARGUMENT)\n",
    "forcing=['date','precipitation', 'peti', 'temperature']\n",
    "target=['date', 'discharge_spec']\n",
    "\n",
    "# time periods\n",
    "training_period = ['1987-10-01','1999-09-30']\n",
    "testing_period = ['2005-10-01','2012-09-30']\n",
    "warmup_period = 365\n",
    "\n",
    "# optimization hyperparameters\n",
    "optimization_hyperparameters = {\n",
    "    \"no_of_epochs\": 40,\n",
    "    \"learning_rate\": 0.05\n",
    "    }\n",
    "\n",
    "# optimization ranges\n",
    "parameter_ranges = {\n",
    "    'aux_ET' : [0.0,1.5],\n",
    "    'ki'    : [1.0, 500.0]\n",
    "}\n",
    "\n",
    "# initial states of the reservoirs (not too important because there is 1 year of warmup)\n",
    "initial_states = {\n",
    "    'si_0'  : 5.0\n",
    "}\n",
    "\n",
    "# Read information from the basins that will be optimize\n",
    "selected_basins_id= list(np.loadtxt(path_basins, skiprows=1).astype(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folder '../results/models/Bucket2/' created successfully.\n"
     ]
    }
   ],
   "source": [
    "# Check if the path where one will store the results exists. In case it does not, it creates such path.\n",
    "if not os.path.exists(path_output):\n",
    "    # Create the folder\n",
    "    os.makedirs(path_output)\n",
    "    print(f\"Folder '{path_output}' created successfully.\")\n",
    "else:\n",
    "    print(f\"Folder '{path_output}' already exists.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataframe to store the calibration results\n",
    "columns_name = ['basin_id', 'NSE_training', 'aux_ET', 'ki', 'NSE_testing'] \n",
    "df_calibration = pd.DataFrame(index=range(len(selected_basins_id)), columns=columns_name)\n",
    "\n",
    "# Lists to store information during optimization\n",
    "SHM_models = []\n",
    "train_datasets = []\n",
    "test_datasets = []\n",
    "\n",
    "# Loop to go through each basin that will be calibrated\n",
    "for i, basin in enumerate(selected_basins_id):\n",
    "    # Dataset for training\n",
    "    train_datasets.append(Forcing_DataSet(basin_id=basin,\n",
    "                                          forcing=forcing,\n",
    "                                          target=target,\n",
    "                                          time_period=training_period,\n",
    "                                          path_ts=path_ts))\n",
    "\n",
    "    # DataLoader for training\n",
    "    year_batches = train_datasets[i].year_batches([9, 30, 0]) # make the year-cut on september\n",
    "    train_loader = DataLoader(train_datasets[i], \n",
    "                              sampler=CustomSampler(year_batches = year_batches),\n",
    "                              collate_fn=collate_fn)\n",
    "\n",
    "\n",
    "    # Define model to be calibrated\n",
    "    SHM_models.append(SHM(parameter_ranges))\n",
    "    states = torch.tensor([i for i in initial_states.values()], dtype=torch.float32)\n",
    "    # Define optimizer\n",
    "    optimizer = torch.optim.Adam(SHM_models[i].parameters(), lr=optimization_hyperparameters['learning_rate'])\n",
    "    \n",
    "    # Training --------------------------------------------------\n",
    "    for epoch in range(optimization_hyperparameters['no_of_epochs']): \n",
    "        q_sim, avg_loss= train_model(data_loader=train_loader, \n",
    "                                     model=SHM_models[i],\n",
    "                                     loss_function=nse_loss,\n",
    "                                     optimizer=optimizer, \n",
    "                                     initial_states=states,\n",
    "                                     warmup_period=warmup_period)\n",
    "        \n",
    "        print(f'Epoch: {epoch + 1:<2} | Loss: {\"%.4f \"% avg_loss}')\n",
    "\n",
    "    # Calculate NSE for whole training set (the loss during the previous loop was calculated by\n",
    "    # batches).\n",
    "    temp = [year_batches[0] , sum(year_batches[1:], [])] # one year for warmup and the rest to calculate the loss\n",
    "    loader_2 = DataLoader(train_datasets[i], sampler=CustomSampler(year_batches = temp),collate_fn=collate_fn)\n",
    "    X_training, y_training = next(iter(loader_2))\n",
    "    # run model\n",
    "    q_sim, _ = SHM_models[i].run_SHM(X_training, states)\n",
    "    # calculate training loss\n",
    "    nse_training = np.round(1.0 - nse_loss(q_sim[warmup_period:], y_training[warmup_period:]).detach().numpy(),3)\n",
    "    # get the optimized parameters\n",
    "    parameters = np.round(SHM_models[i].calibrated_parameters().detach().numpy(),3)\n",
    "\n",
    "    \n",
    "    # Testing --------------------------------------------------\n",
    "    test_datasets.append(Forcing_DataSet(basin_id=basin,\n",
    "                                         forcing=forcing,\n",
    "                                         target=target,\n",
    "                                         time_period=testing_period,\n",
    "                                         path_ts=path_ts))\n",
    "    \n",
    "    year_batches = test_datasets[i].year_batches([9, 30, 0])\n",
    "    temp = [year_batches[0] , sum(year_batches[1:], [])]\n",
    "    loader_2 = DataLoader(test_datasets[i], sampler=CustomSampler(year_batches = temp), collate_fn=collate_fn)\n",
    "    X_testing, y_testing = next(iter(loader_2))\n",
    "    # run model\n",
    "    q_sim, _ = SHM_models[i].run_SHM(X_testing, states)\n",
    "    # calculate testing loss\n",
    "    nse_testing = np.round(1.0 - nse_loss(q_sim[warmup_period:], y_testing[warmup_period:]).detach().numpy(),3)\n",
    "    \n",
    "    # Save the results\n",
    "    row_data = [basin, nse_training] + list(parameters) + [nse_testing]\n",
    "    df_calibration.loc[i] = row_data\n",
    "    print('Calibration of basin:'+str(i+1)+'/'+str(len(selected_basins_id))+' with ID:'+str(basin)+' is completed-------------------------------')\n",
    "\n",
    "# Save all the results in a dataframe\n",
    "df_calibration.to_csv(path_output+'Bucket_SGD_calibration.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "35a50820829c02ecd58b03396d4cb9cc75e0372cdbeb523e83dfcac53b15f211"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
