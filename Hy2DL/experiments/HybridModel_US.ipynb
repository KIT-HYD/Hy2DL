{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook to create a Hybrid Hydrological Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Description**\n",
    "\n",
    "The following notebook contains the code to create, train, validate and test a Hybrid Hydrological Model \n",
    "using a LSTM network plus a process based rainfall-runoff model. The code allows for the creation of single-basin \n",
    "models, but it is conceptualized to create regional models. The code is intended as an intial introduction to the topic, \n",
    "in which we prioritized interpretability over modularity.\n",
    "\n",
    "The logic of the code is heavily based on [Neural Hydrology](https://doi.org/10.21105/joss.04050)[1]. For a more \n",
    "flexible, robust and modular implementation of deep learning method in hydrological modeling we advice the use of Neural \n",
    "Hydrology. \n",
    "\n",
    "**Authors:**\n",
    "- Eduardo Acuna Espinoza (eduardo.espinoza@kit.edu)\n",
    "- Ralf Loritz\n",
    "- Manuel √Ålvarez Chaves\n",
    "\n",
    "**References:**\n",
    "\n",
    "[1]: \"F. Kratzert, M. Gauch, G. Nearing and D. Klotz: NeuralHydrology -- A Python library for Deep Learning research in hydrology. Journal of Open Source Software, 7, 4050, doi: 10.21105/joss.04050, 2022\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import necessary packages\n",
    "import sys\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "sys.path.append(\"../aux_functions\")\n",
    "sys.path.append(\"../datasetzoo\")\n",
    "sys.path.append(\"../modelzoo\")\n",
    "\n",
    "# Import classes and functions from other files\n",
    "from functions_training import weighted_rmse\n",
    "from functions_evaluation import nse\n",
    "from functions_aux import create_folder, set_random_seed, write_report\n",
    "\n",
    "# Import dataset to use\n",
    "from camelsus import CAMELS_US\n",
    "\n",
    "# Import classes that will be used to create the models\n",
    "from hbv import HBV as conceptual_model\n",
    "from uh_routing import UH_routing as routing_model\n",
    "from hybrid import Hybrid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 1. Initialize information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# paths to access the information\n",
    "path_entities = \"../../data/basin_id/basins_camels_us_531.txt\"\n",
    "path_data = \"../../data/CAMELS_US\"\n",
    "path_additional_features = \"../../data/CAMELS_US/pet_hargreaves.pickle\"\n",
    "\n",
    "# dynamic forcings and target\n",
    "dynamic_input = [\"prcp(mm/day)\", \"srad(W/m2)\", \"tmax(C)\", \"tmin(C)\" , \"vp(Pa)\", \"dayl(s)\"]\n",
    "conceptual_input = [\"prcp(mm/day)\", \"pet(mm/day)\", \"tmax(C)\", \"tmin(C)\"]\n",
    "forcings = [\"daymet\"]\n",
    "target = [\"QObs(mm/d)\"]\n",
    "\n",
    "# static attributes that will be used\n",
    "static_input = [\"p_mean\",\n",
    "                \"pet_mean\",\n",
    "                \"p_seasonality\",\n",
    "                \"frac_snow\",\n",
    "                \"aridity\",\n",
    "                \"high_prec_freq\",\n",
    "                \"high_prec_dur\",\n",
    "                \"low_prec_freq\",\n",
    "                \"low_prec_dur\",\n",
    "                \"elev_mean\",\n",
    "                \"slope_mean\",\n",
    "                \"area_gages2\",\n",
    "                \"frac_forest\",\n",
    "                \"lai_max\",\n",
    "                \"lai_diff\",\n",
    "                \"gvf_max\", \n",
    "                \"gvf_diff\",\n",
    "                \"dom_land_cover_frac\",\n",
    "                \"dom_land_cover\",\n",
    "                \"root_depth_50\",\n",
    "                \"soil_depth_pelletier\",\n",
    "                \"soil_depth_statsgo\",\n",
    "                \"soil_porosity\",\n",
    "                \"soil_conductivity\",\n",
    "                \"max_water_content\",\n",
    "                \"sand_frac\",\n",
    "                \"silt_frac\",\n",
    "                \"clay_frac\",\n",
    "                \"geol_1st_class\",\n",
    "                \"glim_1st_class_frac\",\n",
    "                \"geol_2nd_class\",\n",
    "                \"glim_2nd_class_frac\",\n",
    "                \"carbonate_rocks_frac\",\n",
    "                \"geol_porostiy\",\n",
    "                \"geol_permeability\"]\n",
    "\n",
    "# time periods\n",
    "training_period = [\"1980-10-01\",\"1995-09-30\"]\n",
    "validation_period = [\"1980-10-01\",\"1985-09-30\"]\n",
    "testing_period = [\"1995-10-01\",\"2010-09-30\"]\n",
    "\n",
    "model_hyper_parameters = {\n",
    "  \"input_size_lstm\": len(dynamic_input) + len(static_input),\n",
    "  \"no_of_layers\": 1,\n",
    "  \"seq_length\": 730,\n",
    "  \"predict_last_n\": 365,\n",
    "  \"hidden_size\": 256,\n",
    "  \"batch_size\": 256,\n",
    "  \"no_of_epochs\": 20,\n",
    "  \"max_updates_per_epoch\": 450,\n",
    "  \"learning_rate\": 0.001,\n",
    "  \"set_forget_gate\": 3.0,\n",
    "  \"n_conceptual_models\": 16,\n",
    "  \"conceptual_dynamic_parameterization\": [\"BETA\",\"BETAET\"]\n",
    "                     }\n",
    "# device\n",
    "running_device = \"cpu\" #cpu or gpu\n",
    "\n",
    "# define seed\n",
    "seed = 111111\n",
    "\n",
    "# Name of the folder where the results will be stored (the folder must be created before running\n",
    "# the code)\n",
    "path_save_folder = \"../results/Hybrid_US\"\n",
    "\n",
    "# colorblind friendly palette for plotting\n",
    "color_palette = {\"observed\": \"#1f78b4\",\"simulated\": \"#ff7f00\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create folder to store the results\n",
    "create_folder(folder_path=path_save_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 2. Create the different datasets and dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset training\n",
    "training_dataset = CAMELS_US(dynamic_input= dynamic_input,\n",
    "                             forcing= forcings,\n",
    "                             target= target, \n",
    "                             sequence_length= model_hyper_parameters[\"seq_length\"],\n",
    "                             time_period= training_period,\n",
    "                             path_data= path_data,\n",
    "                             path_entities= path_entities,\n",
    "                             path_additional_features= path_additional_features,\n",
    "                             predict_last_n= model_hyper_parameters[\"predict_last_n\"],\n",
    "                             static_input= static_input,\n",
    "                             conceptual_input= conceptual_input,\n",
    "                             check_NaN= True)\n",
    "\n",
    "training_dataset.calculate_basin_std()\n",
    "training_dataset.calculate_global_statistics(path_save_scaler=path_save_folder) # the global statistics are calculated in the training period!\n",
    "training_dataset.standardize_data(standardize_output=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataloader training\n",
    "train_loader = DataLoader(training_dataset, \n",
    "                          batch_size=model_hyper_parameters[\"batch_size\"],\n",
    "                          shuffle=True,\n",
    "                          drop_last = True)\n",
    "\n",
    "print(\"Batches in training: \", len(train_loader))\n",
    "sample = next(iter(train_loader))\n",
    "print(f'x_lstm: {sample[\"x_lstm\"].shape} | x_conceptual: {sample[\"x_conceptual\"].shape} | y_obs: {sample[\"y_obs\"].shape} | basin_std: {sample[\"basin_std\"].shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 3. Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if model will be run in gpu or cpu\n",
    "if running_device == \"gpu\":\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "    device= f'cuda:0'\n",
    "elif running_device == \"cpu\":\n",
    "    device = \"cpu\"\n",
    "\n",
    "# set random seed\n",
    "set_random_seed(seed=seed)\n",
    "\n",
    "# construct model\n",
    "# construct model\n",
    "hybrid_model = Hybrid(hyperparameters=model_hyper_parameters, \n",
    "                      conceptual_model = conceptual_model,\n",
    "                      routing_model = routing_model).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 6. Train LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer: Adam, learning rate, weight decay\n",
    "optimizer = torch.optim.Adam(hybrid_model.parameters(), lr = model_hyper_parameters[\"learning_rate\"])\n",
    "\n",
    "# set forget gate to 3 to ensure that the model is capable to learn long term dependencies\n",
    "hybrid_model.lstm.bias_hh_l0.data[model_hyper_parameters[\"hidden_size\"]:2 * model_hyper_parameters[\"hidden_size\"]] = model_hyper_parameters[\"set_forget_gate\"]\n",
    "\n",
    "training_time = time.time()\n",
    "for epoch in range(model_hyper_parameters[\"no_of_epochs\"]):\n",
    "    epoch_start_time = time.time()\n",
    "    total_loss = []\n",
    "    # Training ----------------------------------------------------------------------\n",
    "    hybrid_model.train()\n",
    "    for idx, sample in enumerate(train_loader): \n",
    "        # maximum iterations per epoch\n",
    "        if idx >= model_hyper_parameters[\"max_updates_per_epoch\"]: break\n",
    "        \n",
    "        optimizer.zero_grad() # sets gradients of weigths and bias to zero\n",
    "        pred = hybrid_model(x_lstm= sample[\"x_lstm\"].to(device), \n",
    "                            x_conceptual= sample[\"x_conceptual\"].to(device)) # forward call\n",
    "        \n",
    "        loss = weighted_rmse(y_sim = pred[\"y_hat\"], \n",
    "                             y_obs = sample[\"y_obs\"].to(device)) \n",
    "\n",
    "        loss.backward() # backpropagates\n",
    "        torch.nn.utils.clip_grad_norm_(hybrid_model.parameters(), 1) #clip gradients\n",
    "        optimizer.step() #update weights\n",
    "        total_loss.append(loss.item())\n",
    "        \n",
    "        # remove from cuda\n",
    "        torch.cuda.empty_cache()\n",
    "        del sample[\"x_lstm\"], sample[\"x_conceptual\"], sample[\"y_obs\"], pred\n",
    "    \n",
    "    # Average loss training\n",
    "    average_loss_training = np.mean(total_loss)\n",
    "         \n",
    "    # save model after every epoch\n",
    "    path_saved_model = path_save_folder+\"/epoch_\" + str(epoch+1)\n",
    "    torch.save(hybrid_model.state_dict(), path_saved_model)\n",
    "            \n",
    "    # print epoch report\n",
    "    epoch_training_time = time.time()-epoch_start_time\n",
    "    report = f'Epoch: {epoch+1:<2} | Loss training: {\"%.3f \"% (average_loss_training)} | Training time: {\"%.1f \"% (epoch_training_time)} s'\n",
    "    print(report)\n",
    "    # save epoch report in txt file\n",
    "    write_report(file_path=path_save_folder+\"/run_progress.txt\", text=report)\n",
    "\n",
    "# print total report\n",
    "total_training_time = time.time()-training_time\n",
    "report = f'Total training time: {\"%.1f \"% (total_training_time)} s'\n",
    "print(report)\n",
    "# save total report in txt file\n",
    "write_report(file_path=path_save_folder+\"/run_progress.txt\", text=report) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 7. Testing period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In case I already trained an LSTM I can re-construct the model\n",
    "#hybrid_model.load_state_dict(torch.load(path_save_folder + \"/epoch_20\", map_location=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# When we run our testing period, we want to differentiate between basins. Therefore, each batch entity will contain\n",
    "# the whole testing period of a specific basin. For this, we will modify seq_length and the predict_last_n.\n",
    "warmup_start_date = pd.to_datetime(testing_period[0],format=\"%Y-%m-%d\") - pd.DateOffset(model_hyper_parameters[\"seq_length\"]-model_hyper_parameters[\"predict_last_n\"])\n",
    "testing_seq_length = (pd.to_datetime(testing_period[1],format=\"%Y-%m-%d\") - warmup_start_date).days + 1 \n",
    "testing_predict_last_n = testing_seq_length-model_hyper_parameters[\"predict_last_n\"]\n",
    "\n",
    "test_dataset = CAMELS_US(dynamic_input= dynamic_input,\n",
    "                         forcing= forcings,\n",
    "                         target= target, \n",
    "                         sequence_length= testing_seq_length,\n",
    "                         time_period= testing_period,\n",
    "                         path_data= path_data,\n",
    "                         path_entities= path_entities,\n",
    "                         path_additional_features= path_additional_features,\n",
    "                         predict_last_n= testing_predict_last_n,\n",
    "                         static_input= static_input,\n",
    "                         conceptual_input= conceptual_input,\n",
    "                         check_NaN= True)\n",
    "\n",
    "test_dataset.scaler = training_dataset.scaler # read the global statistics calculated in the training period\n",
    "test_dataset.standardize_data(standardize_output=False)\n",
    "\n",
    "# DataLoader testing\n",
    "testing_batch_size = 100\n",
    "test_loader = DataLoader(test_dataset, \n",
    "                         batch_size=testing_batch_size,\n",
    "                         shuffle=False,\n",
    "                         drop_last = False)\n",
    "\n",
    "# see if the batches are loaded correctly\n",
    "print(\"Batches in testing: \", len(test_loader))\n",
    "sample = next(iter(test_loader))\n",
    "print(f'x_lstm: {sample[\"x_lstm\"].shape} | x_conceptual: {sample[\"x_conceptual\"].shape} | y_obs: {sample[\"y_obs\"].shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing ----------------------------------------------------------------------\n",
    "warmup_period = model_hyper_parameters[\"seq_length\"]-model_hyper_parameters[\"predict_last_n\"]\n",
    "hybrid_model.eval()\n",
    "test_results={}\n",
    "with torch.no_grad():\n",
    "    for sample in test_loader:\n",
    "        pred = hybrid_model(x_lstm= sample[\"x_lstm\"].to(device), \n",
    "                            x_conceptual= sample[\"x_conceptual\"].to(device)) # forward call\n",
    "\n",
    "        # join results in a dataframe and store them in a dictionary (is easier to plot later)\n",
    "        for i in range(pred[\"y_hat\"].shape[0]):\n",
    "            df_ts = test_dataset.df_ts[sample[\"basin\"][i]].iloc[warmup_period:]\n",
    "            \n",
    "            df_discharge = pd.DataFrame(data={\"y_obs\": sample[\"y_obs\"][i,:,0].flatten().cpu().detach().numpy(), \n",
    "                                              \"y_sim\": pred[\"y_hat\"][i,:,0].flatten().cpu().detach().numpy()}, \n",
    "                                              index=df_ts.index)\n",
    "            \n",
    "            df_discharge = pd.concat([df_ts, df_discharge], axis=1)\n",
    "            df_discharge = df_discharge.filter([\"y_obs\", \"y_sim\"])\n",
    "               \n",
    "            # extract internal_state (buckets) information\n",
    "            internal_states = {key: value[i,:,:].squeeze(0).cpu().detach().numpy() for key, value in pred[\"internal_states\"].items()}\n",
    "        \n",
    "            # extract parameter  information\n",
    "            parameters = {key: value[i,:,:].squeeze(0).cpu().detach().numpy() for key, value in pred[\"parameters\"].items()}\n",
    "        \n",
    "            test_results[sample[\"basin\"][i]] = {\"discharges\": df_discharge,\n",
    "                                                \"internal_states\": internal_states,\n",
    "                                                \"parameters\": parameters}\n",
    "            \n",
    "        # remove from cuda\n",
    "        del pred\n",
    "        torch.cuda.empty_cache()   \n",
    "\n",
    "# Save results as a pickle file\n",
    "#with open(path_save_folder+\"/test_results.pickle\", \"wb\") as f:\n",
    " #   pickle.dump(test_results, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 8. Initial analysis of the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discharges =  {key: value[\"discharges\"] for key, value in test_results.items()}\n",
    "loss_testing = nse(df_results=discharges, average=False)\n",
    "df_NSE = pd.DataFrame(data={\"basin_id\": list(test_results.keys()), \"NSE\": np.round(loss_testing,3)})\n",
    "df_NSE = df_NSE.set_index(\"basin_id\")\n",
    "df_NSE.to_csv(path_save_folder+\"/NSE.csv\", index=True, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the histogram\n",
    "plt.hist(df_NSE[\"NSE\"], bins=[0, .1, .2, .3, .4, .5, .6, .7, .8, .9, 1])\n",
    "\n",
    "# Add NSE statistics to the plot\n",
    "plt.text(0.01, 0.8, f'Mean: {\"%.2f\" % df_NSE[\"NSE\"].mean():>7}\\nMedian: {\"%.2f\" % df_NSE[\"NSE\"].median():>0}\\nMax: {\"%.2f\" % df_NSE[\"NSE\"].max():>9}\\nMin: {\"%.2f\" % df_NSE[\"NSE\"].min():>10}',\n",
    "         transform=plt.gca().transAxes, bbox=dict(facecolor=\"white\", alpha=0.5))\n",
    "\n",
    "# Format plot\n",
    "plt.rcParams[\"figure.figsize\"] = (20, 5)\n",
    "plt.xlabel(\"NSE\", fontsize=12, fontweight=\"bold\")\n",
    "plt.ylabel(\"Frequency\", fontsize=12, fontweight=\"bold\")\n",
    "plt.title(\"NSE Histogram\", fontsize=16, fontweight=\"bold\")\n",
    "#plt.savefig(save_folder+\"/NSE_Histogram.png\", bbox_inches=\"tight\", pad_inches=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot simulated and observed discharges\n",
    "basin_to_analyze = \"01022500\"\n",
    "plt.plot(test_results[basin_to_analyze]['discharges'][\"y_obs\"], label=\"observed\", color=color_palette[\"observed\"])\n",
    "plt.plot(test_results[basin_to_analyze]['discharges'][\"y_sim\"], label=\"simulated\", alpha=0.5, color=color_palette[\"simulated\"])\n",
    "\n",
    "# Format plot\n",
    "plt.xlabel(\"Day\", fontsize=12, fontweight=\"bold\")\n",
    "plt.ylabel(\"Discharge [mm/d]\", fontsize=12, fontweight=\"bold\")\n",
    "plt.title(\"Discharge comparison\", fontsize=16, fontweight=\"bold\")\n",
    "plt.tick_params(axis=\"both\", which=\"major\", labelsize=12)\n",
    "plt.legend(loc=\"upper right\",fontsize=12)\n",
    "#plt.savefig(save_folder+\"/Model_Comparison.png\", bbox_inches=\"tight\", pad_inches=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot states\n",
    "state_of_interest = \"SM\"\n",
    "\n",
    "states = test_results[basin_to_analyze][\"internal_states\"][state_of_interest]\n",
    "\n",
    "for i in range(states.shape[1]):\n",
    "    plt.plot(test_results[basin_to_analyze]['discharges'][\"y_obs\"].index, \n",
    "             states[:, i], \n",
    "             label=state_of_interest+\"_\"+str(i+1))\n",
    "\n",
    "# Adding labels and legend\n",
    "plt.xlabel(\"Day\", fontsize=12, fontweight=\"bold\")\n",
    "plt.ylabel(\"State [mm/d]\", fontsize=12, fontweight=\"bold\")\n",
    "plt.title(\"Time evolution of internal states of conceptual model\", fontsize=16, fontweight=\"bold\")\n",
    "plt.tick_params(axis=\"both\", which=\"major\", labelsize=12)\n",
    "plt.legend(loc=\"upper right\",fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot states\n",
    "parameter_of_interest= \"BETA\"\n",
    "\n",
    "states = test_results[basin_to_analyze][\"parameters\"][parameter_of_interest]\n",
    "\n",
    "for i in range(states.shape[1]):\n",
    "    plt.plot(test_results[basin_to_analyze]['discharges'][\"y_obs\"].index, \n",
    "             states[:, i], \n",
    "             label=parameter_of_interest+\"_\"+str(i+1), alpha=0.8)\n",
    "\n",
    "# Adding labels and legend\n",
    "plt.xlabel(\"Day\", fontsize=12, fontweight=\"bold\")\n",
    "plt.ylabel(\"State [mm/d]\", fontsize=12, fontweight=\"bold\")\n",
    "plt.title(\"Time evolution of parameters of conceptual model\", fontsize=16, fontweight=\"bold\")\n",
    "plt.tick_params(axis=\"both\", which=\"major\", labelsize=12)\n",
    "plt.legend(loc=\"upper right\",fontsize=12)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
