{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook to create a regional rainfall-runoff model using an LSTM network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Description**\n",
    "\n",
    "The following notebook contains the code to create, train, validate and test a rainfall-runoff model using a LSTM \n",
    "network architecture. The code allows for the creation of single-basin models, but it is conceptualized to create \n",
    "regional models. The code is intended as an intial introduction to the topic, in which we prioritized interpretability\n",
    "over modularity.\n",
    "\n",
    "The logic of the code is heavily based on [Neural Hydrology](https://doi.org/10.21105/joss.04050)[1]. For a more \n",
    "flexible, robust and modular implementation of deep learning method in hydrological modeling we advice the use of Neural \n",
    "Hydrology. \n",
    "\n",
    "**Authors:**\n",
    "- Eduardo Acuna Espinoza (eduardo.espinoza@kit.edu)\n",
    "- Ralf Loritz\n",
    "- Manuel √Ålvarez Chaves\n",
    "\n",
    "**References:**\n",
    "\n",
    "[1]: \"F. Kratzert, M. Gauch, G. Nearing and D. Klotz: NeuralHydrology -- A Python library for Deep Learning research in hydrology. Journal of Open Source Software, 7, 4050, doi: 10.21105/joss.04050, 2022\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import necessary packages\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import pickle\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "sys.path.append(\"../aux_functions\")\n",
    "sys.path.append(\"../datasetzoo\")\n",
    "sys.path.append(\"../modelzoo\")\n",
    "\n",
    "# Import classes and functions from other files\n",
    "from functions_training import nse_basin_averaged\n",
    "from functions_evaluation import nse\n",
    "from functions_aux import create_folder, set_random_seed, write_report\n",
    "\n",
    "# Import dataset to use\n",
    "from camelsde import CAMELS_DE\n",
    "\n",
    "# Import model\n",
    "from cudalstm import CudaLSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 1. Initialize information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define experiment nae\n",
    "experiment_name = \"LSTM_CAMELS_DE\"\n",
    "\n",
    "# paths to access the information\n",
    "path_entities = \"../../data/basin_id/basins_camels_de_1583.txt\"\n",
    "path_data = \"../../data/CAMELS_DE\"\n",
    "\n",
    "# dynamic forcings and target\n",
    "dynamic_input = [\"precipitation_mean\", \"precipitation_stdev\", \"radiation_global_mean\", \"temperature_min\", \"temperature_max\"]\n",
    "target = [\"discharge_spec\"]\n",
    "\n",
    "# static attributes that will be used\n",
    "static_input = [\"area\",\n",
    "                \"elev_mean\",\n",
    "                \"clay_0_30cm_mean\",\n",
    "                \"sand_0_30cm_mean\",\n",
    "                \"silt_0_30cm_mean\",\n",
    "                \"artificial_surfaces_perc\",\n",
    "                \"agricultural_areas_perc\",\n",
    "                \"forests_and_seminatural_areas_perc\",\n",
    "                \"wetlands_perc\",\n",
    "                \"water_bodies_perc\",\n",
    "                \"p_mean\",\n",
    "                \"p_seasonality\",\n",
    "                \"frac_snow\",\n",
    "                \"high_prec_freq\",\n",
    "                \"low_prec_freq\",\n",
    "                \"high_prec_dur\",\n",
    "                \"low_prec_dur\"]\n",
    "\n",
    "# time periods\n",
    "training_period = [\"1970-10-01\",\"1999-12-31\"]\n",
    "validation_period = [\"1965-10-01\",\"1970-09-30\"]\n",
    "testing_period = [\"2000-01-01\",\"2020-12-31\"]\n",
    "\n",
    "model_hyper_parameters = {\n",
    "    \"input_size_lstm\": len(dynamic_input) + len(static_input),\n",
    "    \"no_of_layers\":1,  \n",
    "    \"seq_length\": 365,\n",
    "    \"hidden_size\": 128,\n",
    "    \"batch_size_training\":256,\n",
    "    \"batch_size_evaluation\":1024,\n",
    "    \"no_of_epochs\": 20,             \n",
    "    \"drop_out_rate\": 0.4, \n",
    "    \"learning_rate\": 0.001,\n",
    "    \"adapt_learning_rate_epoch\": 10,\n",
    "    \"adapt_gamma_learning_rate\": 0.5,\n",
    "    \"set_forget_gate\":3,\n",
    "    \"validate_every\": 20,\n",
    "    \"validate_n_random_basins\": 100\n",
    "    }\n",
    "\n",
    "# device to train the model\n",
    "running_device = \"gpu\" #cpu or gpu\n",
    "\n",
    "# define random seed\n",
    "seed = 42\n",
    "\n",
    "# colorblind friendly palette for plotting\n",
    "color_palette = {\"observed\": \"#1f78b4\",\"simulated\": \"#ff7f00\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create folder to store the results\n",
    "path_save_folder = \"../results/\"+experiment_name\n",
    "create_folder(folder_path=path_save_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if model will be run in gpu or cpu and define device\n",
    "if running_device == \"gpu\":\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "    device= f'cuda:0'\n",
    "elif running_device == \"cpu\":\n",
    "    device = \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 2. Create dataset and dataloader for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset training\n",
    "training_dataset = CAMELS_DE(dynamic_input= dynamic_input,\n",
    "                             target= target, \n",
    "                             sequence_length= model_hyper_parameters[\"seq_length\"],\n",
    "                             time_period= training_period,\n",
    "                             path_data= path_data,\n",
    "                             path_entities= path_entities,\n",
    "                             static_input= static_input,\n",
    "                             check_NaN= True)\n",
    "\n",
    "training_dataset.calculate_basin_std()\n",
    "training_dataset.calculate_global_statistics(path_save_scaler=path_save_folder)\n",
    "training_dataset.standardize_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataloader training\n",
    "train_loader = DataLoader(dataset = training_dataset, \n",
    "                          batch_size = model_hyper_parameters[\"batch_size_training\"],\n",
    "                          shuffle = True,\n",
    "                          drop_last = True)\n",
    "\n",
    "print(\"Batches in training: \", len(train_loader))\n",
    "sample = next(iter(train_loader))\n",
    "print(f'x_lstm: {sample[\"x_lstm\"].shape} | y_obs: {sample[\"y_obs\"].shape} | basin_std: {sample[\"basin_std\"].shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 3. Create dataset for validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will create an individual dataset per basin. This will give us more flexibility\n",
    "entities_ids = np.loadtxt(path_entities, dtype=\"str\").tolist()\n",
    "validation_dataset = {}\n",
    "\n",
    "for entity in entities_ids:\n",
    "    dataset = CAMELS_DE(dynamic_input= dynamic_input,\n",
    "                        target= target, \n",
    "                        sequence_length= model_hyper_parameters[\"seq_length\"],\n",
    "                        time_period= validation_period,\n",
    "                        path_data= path_data,\n",
    "                        entity= entity,\n",
    "                        static_input= static_input,\n",
    "                        check_NaN= False)\n",
    "    \n",
    "    dataset.scaler = training_dataset.scaler\n",
    "    dataset.standardize_data(standardize_output=False)\n",
    "    validation_dataset[entity]= dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 4. Train LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct model\n",
    "set_random_seed(seed=seed)\n",
    "lstm_model = CudaLSTM(hyperparameters=model_hyper_parameters).to(device)\n",
    "\n",
    "# optimizer\n",
    "optimizer = torch.optim.Adam(lstm_model.parameters(),\n",
    "                             lr=model_hyper_parameters[\"learning_rate\"])\n",
    "    \n",
    "# define learning rate scheduler\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer,\n",
    "                                            step_size=model_hyper_parameters[\"adapt_learning_rate_epoch\"],\n",
    "                                            gamma=model_hyper_parameters[\"adapt_gamma_learning_rate\"])\n",
    "\n",
    "# set forget gate to 3 to ensure that the model is capable to learn long term dependencies\n",
    "lstm_model.lstm.bias_hh_l0.data[model_hyper_parameters[\"hidden_size\"]:2 * model_hyper_parameters[\"hidden_size\"]]=\\\n",
    "    model_hyper_parameters[\"set_forget_gate\"]\n",
    "\n",
    "training_time = time.time()\n",
    "# Loop through the different epochs\n",
    "for epoch in range(1, model_hyper_parameters[\"no_of_epochs\"]+1):\n",
    "    \n",
    "    epoch_start_time = time.time()\n",
    "    total_loss = []\n",
    "    # Training -------------------------------------------------------------------------------------------------------\n",
    "    lstm_model.train()\n",
    "    for sample in train_loader: \n",
    "        optimizer.zero_grad() # sets gradients of weigths and bias to zero\n",
    "        pred  = lstm_model(sample[\"x_lstm\"].to(device)) # forward call\n",
    "        \n",
    "        loss = nse_basin_averaged(y_sim=pred[\"y_hat\"], \n",
    "                                  y_obs=sample[\"y_obs\"].to(device), \n",
    "                                  per_basin_target_std=sample[\"basin_std\"].to(device))\n",
    "        \n",
    "        loss.backward() # backpropagates\n",
    "        torch.nn.utils.clip_grad_norm_(lstm_model.parameters(), 1) #clip gradients\n",
    "        optimizer.step() #update weights\n",
    "        total_loss.append(loss.item())\n",
    "        \n",
    "        # remove from cuda\n",
    "        del sample[\"x_lstm\"], sample[\"y_obs\"], sample[\"basin_std\"], pred\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "    #training report  \n",
    "    report = f'Epoch: {epoch:<2} | Loss training: {\"%.3f \"% (np.mean(total_loss))}'\n",
    "    \n",
    "    # Validation -----------------------------------------------------------------------------------------------------\n",
    "    if epoch % model_hyper_parameters[\"validate_every\"] == 0:\n",
    "        lstm_model.eval()\n",
    "        validation_results = {}\n",
    "        with torch.no_grad():\n",
    "            # If we define validate_n_random_basins as 0 or negative, we take all the basins\n",
    "            if model_hyper_parameters[\"validate_n_random_basins\"] <= 0:\n",
    "                validation_basin_ids = validation_dataset.keys()\n",
    "            else:\n",
    "                keys = list(validation_dataset.keys())\n",
    "                validation_basin_ids = random.sample(keys, model_hyper_parameters[\"validate_n_random_basins\"])\n",
    "            \n",
    "            # go through each basin that will be used for validation\n",
    "            for basin in validation_basin_ids:\n",
    "                loader = DataLoader(dataset=validation_dataset[basin], \n",
    "                                    batch_size=model_hyper_parameters[\"batch_size_evaluation\"], \n",
    "                                    shuffle=False, \n",
    "                                    drop_last = False)\n",
    "                \n",
    "                df_ts = pd.DataFrame()\n",
    "                for sample in loader:\n",
    "                    pred  = lstm_model(sample[\"x_lstm\"].to(device)) \n",
    "                    # backtransformed information\n",
    "                    y_sim = pred[\"y_hat\"]* validation_dataset[basin].scaler[\"y_std\"].to(device) +\\\n",
    "                        validation_dataset[basin].scaler[\"y_mean\"].to(device)\n",
    "\n",
    "                    # join results in a dataframe and store them in a dictionary (is easier to plot later)\n",
    "                    df = pd.DataFrame({\"y_obs\": sample[\"y_obs\"].flatten().cpu().detach(), \n",
    "                                       \"y_sim\": y_sim.flatten().cpu().detach()}, \n",
    "                                      index=pd.to_datetime(sample[\"date\"]))\n",
    "\n",
    "                    df_ts = pd.concat([df_ts, df], axis=0)\n",
    "\n",
    "                    # remove from cuda\n",
    "                    del pred, y_sim\n",
    "                    torch.cuda.empty_cache()       \n",
    "                \n",
    "                validation_results[basin] = df_ts\n",
    "                 \n",
    "            #average loss validation\n",
    "            loss_validation = nse(df_results=validation_results)\n",
    "            report += f'| Loss validation: {\"%.3f \"% (loss_validation)}'\n",
    "\n",
    "    \n",
    "    # save model after every epoch\n",
    "    path_saved_model = path_save_folder+\"/epoch_\" + str(epoch)\n",
    "    torch.save(lstm_model.state_dict(), path_saved_model)\n",
    "            \n",
    "    # print epoch report\n",
    "    report += f'| Epoch time: {\"%.1f \"% (time.time()-epoch_start_time)} s | LR:{\"%.5f \"% (optimizer.param_groups[0][\"lr\"])}'\n",
    "    print(report)\n",
    "    write_report(file_path=path_save_folder+\"/run_progress.txt\", text=report)\n",
    "    # modify learning rate\n",
    "    scheduler.step()\n",
    "\n",
    "# print final report\n",
    "report = f'Total training time: {\"%.1f \"% (time.time()-training_time)} s'\n",
    "print(report)\n",
    "write_report(file_path=path_save_folder+\"/run_progress.txt\", text=report)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 5. Test LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In case I already trained an LSTM I can re-construct the model\n",
    "#lstm_model = CudaLSTM(hyperparameters=model_hyper_parameters).to(device)\n",
    "#lstm_model.load_state_dict(torch.load(path_save_folder + \"/epoch_20\", map_location=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will create an individual dataset per basin. This will give us more flexibility\n",
    "entities_ids = np.loadtxt(path_entities, dtype=\"str\").tolist()\n",
    "testing_dataset = {}\n",
    "\n",
    "# We can read a previously stored scaler or use the one from the training dataset we just generated\n",
    "#scaler = training_dataset.scaler\n",
    "with open(path_save_folder + \"/scaler.pickle\", \"rb\") as file:\n",
    "    scaler = pickle.load(file)\n",
    "\n",
    "for entity in entities_ids:\n",
    "    dataset = CAMELS_DE(dynamic_input= dynamic_input,\n",
    "                        target= target, \n",
    "                        sequence_length= model_hyper_parameters[\"seq_length\"],\n",
    "                        time_period= testing_period,\n",
    "                        path_data= path_data,\n",
    "                        entity= entity,\n",
    "                        static_input= static_input,\n",
    "                        check_NaN= False)\n",
    "    \n",
    "    dataset.scaler = scaler\n",
    "    dataset.standardize_data(standardize_output=False)\n",
    "    testing_dataset[entity]= dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_model.eval()\n",
    "test_results = {}\n",
    "with torch.no_grad():\n",
    "    for basin, dataset in testing_dataset.items():\n",
    "        loader = DataLoader(dataset = dataset, \n",
    "                            batch_size = model_hyper_parameters[\"batch_size_evaluation\"], \n",
    "                            shuffle = False, \n",
    "                            drop_last = False) \n",
    "        df_ts = pd.DataFrame()\n",
    "        for sample in loader:\n",
    "            pred  = lstm_model(sample[\"x_lstm\"].to(device)) \n",
    "            # backtransformed information\n",
    "            y_sim = pred[\"y_hat\"]* dataset.scaler[\"y_std\"].to(device) + dataset.scaler[\"y_mean\"].to(device)\n",
    "\n",
    "            # join results in a dataframe and store them in a dictionary (is easier to plot later)\n",
    "            df = pd.DataFrame({\"y_obs\": sample[\"y_obs\"].flatten().cpu().detach(), \n",
    "                                \"y_sim\": y_sim.flatten().cpu().detach()}, \n",
    "                                index=pd.to_datetime(sample[\"date\"]))\n",
    "\n",
    "            df_ts = pd.concat([df_ts, df], axis=0)\n",
    "\n",
    "            # remove from cuda\n",
    "            del pred, y_sim\n",
    "            torch.cuda.empty_cache()       \n",
    "        \n",
    "        test_results[basin] = df_ts\n",
    "\n",
    "# Save results as a pickle file\n",
    "#with open(path_save_folder+\"/test_results.pickle\", \"wb\") as f:\n",
    "#    pickle.dump(test_results, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 6. Initial analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In case we already ran our model\n",
    "#with open(path_save_folder+\"/test_results.pickle\", \"rb\") as f:\n",
    "#    test_results = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss testing\n",
    "loss_testing = nse(df_results=test_results, average=False)\n",
    "df_NSE = pd.DataFrame(data={\"basin_id\": test_results.keys(), \"NSE\": np.round(loss_testing,3)})\n",
    "df_NSE = df_NSE.set_index(\"basin_id\")\n",
    "df_NSE.to_csv(path_save_folder+\"/NSE.csv\", index=True, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Plot the histogram\n",
    "plt.hist(df_NSE[\"NSE\"], bins=[0, .1, .2, .3, .4, .5, .6, .7, .8, .9, 1])\n",
    "\n",
    "# Add NSE statistics to the plot\n",
    "plt.text(0.01, 0.8, f'Mean: {\"%.3f\" % df_NSE[\"NSE\"].mean():>7}\\nMedian: {\"%.3f\" % df_NSE[\"NSE\"].median():>0}\\nMax: {\"%.2f\" % df_NSE[\"NSE\"].max():>9}\\nMin: {\"%.2f\" % df_NSE[\"NSE\"].min():>10}',\n",
    "         transform=plt.gca().transAxes, bbox=dict(facecolor=\"white\", alpha=0.5))\n",
    "\n",
    "# Format plot\n",
    "plt.rcParams[\"figure.figsize\"] = (20, 5)\n",
    "plt.xlabel(\"NSE\", fontsize=12, fontweight=\"bold\")\n",
    "plt.ylabel(\"Frequency\", fontsize=12, fontweight=\"bold\")\n",
    "plt.title(\"NSE Histogram\", fontsize=16, fontweight=\"bold\")\n",
    "#plt.savefig(save_folder+\"/NSE_LSTM_Histogram.png\", bbox_inches=\"tight\", pad_inches=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot simulated and observed discharges\n",
    "basin_to_analyze = \"DE710030\"\n",
    "\n",
    "plt.plot(test_results[basin_to_analyze][\"y_obs\"], label=\"observed\", color=color_palette[\"observed\"])\n",
    "plt.plot(test_results[basin_to_analyze][\"y_sim\"], label=\"simulated\", alpha=0.5, color=color_palette[\"simulated\"])\n",
    "\n",
    "# Format plot\n",
    "plt.xlabel(\"Day\", fontsize=12, fontweight=\"bold\")\n",
    "plt.ylabel(\"Discharge [mm/d]\", fontsize=12, fontweight=\"bold\")\n",
    "plt.title(\"Discharge comparison\", fontsize=16, fontweight=\"bold\")\n",
    "plt.tick_params(axis=\"both\", which=\"major\", labelsize=12)\n",
    "plt.legend(loc=\"upper right\",fontsize=12)\n",
    "#plt.savefig(save_folder+\"/Model_Comparison.png\", bbox_inches=\"tight\", pad_inches=0)"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "pytorch-gpu.2-0.m111",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.2-0:m111"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "vscode": {
   "interpreter": {
    "hash": "42b7dc197ee81dd2f6541889b0e14556b882d218c1e7c97db94bc0f7b191f034"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
