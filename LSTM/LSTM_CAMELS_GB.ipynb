{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook to create a regional rainfall-runoff model using an LSTM network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Description**\n",
    "\n",
    "The following notebook contains the code to create, train, validate and test a rainfall-runoff model using a LSTM \n",
    "network architecture. The code allows for the creation of single-basin models, but it is conceptualized to create \n",
    "regional models. The code is intended as an intial introduction to the topic, in which we prioritized interpretability\n",
    "over modularity.\n",
    "\n",
    "The logic of the code is heavily based on [Neural Hydrology](https://doi.org/10.21105/joss.04050)[1]. For a more \n",
    "flexible, robust and modular implementation of deep learning method in hydrological modeling we advice the use of Neural \n",
    "Hydrology. \n",
    "\n",
    "**Authors:**\n",
    "- Eduardo Acuna Espinoza (eduardo.espinoza@kit.edu)\n",
    "- Ralf Loritz\n",
    "- Manuel √Ålvarez Chaves\n",
    "\n",
    "**References:**\n",
    "\n",
    "[1]: \"F. Kratzert, M. Gauch, G. Nearing and D. Klotz: NeuralHydrology -- A Python library for Deep Learning research in hydrology. Journal of Open Source Software, 7, 4050, doi: 10.21105/joss.04050, 2022\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import necessary packages\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from typing import List, Dict\n",
    "from itertools import groupby\n",
    "\n",
    "sys.path.append(\"../aux_functions\")\n",
    "from functions_datasets import CAMELS_GB as DataBase # define what you import as DataBase!\n",
    "from functions_datasets import validate_samples \n",
    "from functions_loss import nse_loss, nse_basin_averaged_loss\n",
    "from functions_aux import create_folder, set_random_seed, write_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 1. Initialize information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# paths to access the information\n",
    "path_entities = '../data/basin_id/basins_camels_gb_669.txt'\n",
    "path_data = '../data/CAMELS_GB'\n",
    "\n",
    "# dynamic forcings and target\n",
    "dynamic_input = ['precipitation', 'peti', 'temperature']\n",
    "target = ['discharge_spec']\n",
    "\n",
    "# static attributes that will be used\n",
    "static_input = ['area',\n",
    "                'elev_mean',\n",
    "                'dpsbar',\n",
    "                'sand_perc',\n",
    "                'silt_perc',\n",
    "                'clay_perc',\n",
    "                'porosity_hypres',\n",
    "                'conductivity_hypres',\n",
    "                'soil_depth_pelletier',\n",
    "                'dwood_perc',\n",
    "                'ewood_perc',\n",
    "                'crop_perc',\n",
    "                'urban_perc',\n",
    "                'reservoir_cap',\n",
    "                'p_mean',\n",
    "                'pet_mean',\n",
    "                'p_seasonality',\n",
    "                'frac_snow',\n",
    "                'high_prec_freq',\n",
    "                'low_prec_freq',\n",
    "                'high_prec_dur',\n",
    "                'low_prec_dur']\n",
    "\n",
    "# time periods\n",
    "training_period = ['1980-10-01','1997-12-31']\n",
    "validation_period = ['1975-10-01','1980-09-30']\n",
    "testing_period = ['1998-01-01','2008-12-31']\n",
    "\n",
    "model_hyper_parameters = {\n",
    "    \"input_size\": len(dynamic_input) + len(static_input),\n",
    "    \"no_of_layers\":1,  \n",
    "    \"seq_length\": 365,\n",
    "    \"hidden_size\": 64,\n",
    "    \"batch_size\":256,\n",
    "    \"no_of_epochs\": 20,             \n",
    "    \"drop_out\": 0.4, \n",
    "    \"learning_rate\": 0.001,\n",
    "    \"adapt_learning_rate_epoch\": 10,\n",
    "    \"adapt_gamma_learning_rate\": 0.5,\n",
    "    \"set_forget_gate\":3\n",
    "}\n",
    "\n",
    "# device to train the model\n",
    "running_device = 'gpu' #cpu or gpu\n",
    "# define random seed\n",
    "seed = 42\n",
    "# Name of the folder where the results will be stored \n",
    "path_save_folder = '../results/models/LSTM_CAMELS_GB'\n",
    "\n",
    "# colorblind friendly palette for plotting\n",
    "color_palette = {'observed': '#1f78b4','LSTM': '#ff7f00'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create folder to store the results\n",
    "create_folder(folder_path=path_save_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 2. Class to create the dataset object used to manage the information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseDataset(Dataset):\n",
    "    \"\"\"Base data set class to load and preprocess data.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dynamic_inputs : List[str]\n",
    "        name of variables used as dynamic series input in the lstm\n",
    "    static_inputs : List[str]\n",
    "        static inputs used as input in the lstm (e.g. catchment attributes)\n",
    "    target: List[str]\n",
    "        target variable(s)\n",
    "    sequence_length: int\n",
    "        sequence length used in the LSTM\n",
    "    time_period: List[str]\n",
    "        initial and final date (e.g. ['1987-10-01','1999-09-30']) of the time period of interest \n",
    "    path_entities: str\n",
    "        path to a txt file that contain the id of the entities (e.g. catchment`s ids) that will be analyzed\n",
    "    path_data: str\n",
    "        path to the folder were the data is stored\n",
    "    path_addional features: str\n",
    "        Optional parameter. Allows the option to add any arbitrary data that is not included in the standard data sets.\n",
    "        Path to a pickle file (or list of paths for multiple files), containing a dictionary with each key corresponding \n",
    "        to one basin id and the value is a date-time indexed pandas DataFrame.      \n",
    "    forcing: List[str]\n",
    "        For CAMELS-US dataset we should specificy which forcing data will be used (e.g. daymet, maurer, ndlas, etc.)\n",
    "    check_Nan: bool\n",
    "        Boolean that indicate if one should check of NaN values while processing the data\n",
    "    \"\"\"\n",
    "\n",
    "    #Function to initialize the data\n",
    "    def __init__(self, \n",
    "                 dynamic_input: List[str],\n",
    "                 static_input: List[str],\n",
    "                 target: List[str], \n",
    "                 sequence_length: int,\n",
    "                 time_period: List[str],\n",
    "                 path_entities: str,\n",
    "                 path_data: str,\n",
    "                 path_additional_features: str = '',\n",
    "                 forcings: List[str] = [],\n",
    "                 check_NaN:bool = True\n",
    "                 ):\n",
    "\n",
    "        # read and create variables\n",
    "        self.time_period = time_period # time period that is being considered\n",
    "        self.dynamic_input= dynamic_input  # dynamic forcings going as inputs of in the lstm\n",
    "        self.target = target  # target variable\n",
    "        self.sequence_length = sequence_length # sequence length\n",
    "\n",
    "        entities_ids = np.loadtxt(path_entities, dtype='str').tolist() \n",
    "        # save the cathments as a list even if there is just one\n",
    "        self.entities_ids = [entities_ids] if isinstance(entities_ids, str) else entities_ids # catchments\n",
    "\n",
    "        self.sequence_data = {} # store information that will be used to run the lstm\n",
    "        self.df_ts = {} # store processed dataframes for all basins\n",
    "        self.scaler = {} # information to standardize the data \n",
    "        self.basin_std = {} # std of the target variable of each basin (can be used later in the loss function)\n",
    "        self.valid_entities= [] # list of the elements that meet the criteria to be used by the lstm\n",
    "\n",
    "        # process the attributes\n",
    "        self.static_input = static_input # static attributes going as inputs to the lstm\n",
    "        if static_input:\n",
    "            self.df_attributes = self._load_attributes(path_data)\n",
    "\n",
    "        # process additional features that will be included in the inputs (optional) ---\n",
    "        if path_additional_features:\n",
    "            self.additional_features = self._load_additional_features(path_additional_features)\n",
    "        \n",
    "        # This loop goes through all the catchments. For each catchment in creates an entry in the dictionary\n",
    "        # self.sequence_data, where we will store the information that will be sent to the lstm\n",
    "        for id in self.entities_ids:\n",
    "            # load time series for specific catchment id\n",
    "            df_ts = self._load_data(path_data=path_data, catch_id=id, forcings = forcings)\n",
    "            # add additional features (optional)\n",
    "            if path_additional_features:\n",
    "                df_ts = pd.concat([df_ts, self.additional_features[id]], axis=1)\n",
    "            \n",
    "            # Defines the start date considering the offset due to sequence length. We want that, if possible, the start\n",
    "            # date is the first date of prediction.\n",
    "            start_date = pd.to_datetime(self.time_period[0],format=\"%Y-%m-%d\")\n",
    "            end_date = pd.to_datetime(self.time_period[1],format=\"%Y-%m-%d\")\n",
    "            freq = pd.infer_freq(df_ts.index)\n",
    "            warmup_start_date = start_date - (self.sequence_length-1)*pd.tseries.frequencies.to_offset(freq)\n",
    "            # filter dataframe for the period and variables of interest\n",
    "            df_ts = df_ts.loc[warmup_start_date:end_date, self.dynamic_input + self.target]\n",
    "            \n",
    "            # reindex the dataframe to assure continuos data between the start and end date of the time period. Missing \n",
    "            # data will be filled with NaN, so this will be taken care of later. \n",
    "            full_range = pd.date_range(start=warmup_start_date, end=end_date, freq=freq)\n",
    "            df_ts = df_ts.reindex(full_range)\n",
    "            \n",
    "            # checks for invalid samples due to NaN or insufficient sequence length\n",
    "            flag = validate_samples(x = df_ts.loc[:, self.dynamic_input].values, \n",
    "                                    y = df_ts.loc[:, self.target].values, \n",
    "                                    attributes = self.df_attributes.loc[id].values if static_input else None, \n",
    "                                    seq_length = self.sequence_length,\n",
    "                                    check_NaN = check_NaN\n",
    "                                    )\n",
    "            \n",
    "            # create a list that contain the indexes (basin, day) of the valid samples\n",
    "            valid_samples = np.argwhere(flag == 1)\n",
    "            self.valid_entities.extend([(id, int(f[0])) for f in valid_samples])\n",
    "            \n",
    "            # only store data if this basin has at least one valid sample in the given period\n",
    "            if valid_samples.size>0:\n",
    "                self.df_ts[id] = df_ts\n",
    "                \n",
    "                # create dictionary entry for the basin\n",
    "                self.sequence_data[id] = {}\n",
    "\n",
    "                # store the information of the basin in a nested dictionary\n",
    "                self.sequence_data[id]['x_d'] = torch.tensor(df_ts.loc[:, self.dynamic_input].values, dtype=torch.float32)\n",
    "                self.sequence_data[id]['y'] = torch.tensor(df_ts.loc[:, self.target].values, dtype=torch.float32)\n",
    "                if self.static_input:\n",
    "                    self.sequence_data[id]['x_s'] = torch.tensor(self.df_attributes.loc[id].values, dtype=torch.float32)\n",
    "                    \n",
    "    def __len__(self):\n",
    "        return len(self.valid_entities)\n",
    "    \n",
    "    def __getitem__(self, id):\n",
    "        \"\"\"This function is used by PyTorch's dataloader to extract the information\"\"\"\n",
    "        basin, i = self.valid_entities[id]\n",
    "\n",
    "        # tensor of inputs\n",
    "        x_LSTM = self.sequence_data[basin]['x_d'][i-self.sequence_length+1:i+1, :]\n",
    "        if self.static_input:\n",
    "            x_s = self.sequence_data[basin]['x_s'].repeat(x_LSTM.shape[0],1)\n",
    "            x_LSTM = torch.cat([x_LSTM, x_s], dim=1)\n",
    "        \n",
    "        # Ttensor of outputs\n",
    "        y_obs = self.sequence_data[basin]['y'][i]\n",
    "\n",
    "        # optional also return the basin_std\n",
    "        if self.basin_std:\n",
    "            return x_LSTM, y_obs, self.basin_std[basin].unsqueeze(0)\n",
    "        else:\n",
    "            return x_LSTM, y_obs\n",
    "\n",
    "    def _load_attributes(self, path_data: str) -> pd.DataFrame:\n",
    "        \"\"\"Call the specific function that reads the static attributes information.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        path_data : str\n",
    "            path to the folder were the data is stored\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        df_attributes: pd.DataFrame\n",
    "            Dataframe containing the attributes of interest for the catchments of interest\n",
    "        \"\"\"\n",
    "        df_attributes = DataBase.read_attributes(path_data=path_data)\n",
    "        df_attributes = df_attributes.loc[self.entities_ids, self.static_input]\n",
    "        return df_attributes\n",
    "\n",
    "    def _load_data(self, path_data: str, catch_id:str, forcings:List[str]) -> pd.DataFrame:\n",
    "        \"\"\"Call the specific function that reads a specific catchment timeseries into a dataframe.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        path_data : str\n",
    "            path to the folder were the data is stored.\n",
    "        catch_id : str\n",
    "            basin_id.\n",
    "        forcings : str\n",
    "            Can be e.g. 'daymet' or 'nldas', etc. Must match the folder names in the 'basin_mean_forcing' directory. \n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        df: pd.DataFrame\n",
    "            Dataframe with the catchments` timeseries\n",
    "        \"\"\"\n",
    "        df_ts = DataBase.read_data(path_data=path_data, catch_id=catch_id, forcings = forcings)\n",
    "        return df_ts\n",
    "\n",
    "    def _load_additional_features(self, path_additional_features: str) -> Dict[str, pd.DataFrame]:\n",
    "        \"\"\"Read pickle dictionary containing additional features.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        path_additional_features : str\n",
    "            Path to a pickle file (or list of paths for multiple files), containing a dictionary with each key \n",
    "            corresponding to one basin id and the value is a date-time indexed pandas DataFrame.   \n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        additional_features: Dict[str, pd.DataFrame]\n",
    "            Dictionary where each key is a basin and each value is a date-time indexed pandas DataFrame with the \n",
    "            additional features\n",
    "        \"\"\"\n",
    "        \n",
    "        with open(path_additional_features, \"rb\") as file:\n",
    "            additional_features = pickle.load(file)\n",
    "        return additional_features\n",
    "  \n",
    "    def calculate_basin_std(self):\n",
    "        \"\"\"Fill the self.basin_std dictionary with the standard deviation of the target variables for each basin\"\"\"\n",
    "        for id, data in self.sequence_data.items():\n",
    "            self.basin_std[id] = torch.tensor(np.nanstd(data['y'].numpy()), dtype=torch.float32)\n",
    "    \n",
    "    def calculate_global_statistics(self):\n",
    "        \"\"\"Fill the self.scalar dictionary \n",
    "        \n",
    "        The function calculates the global mean and standard deviation of the dynamic inputs, target variables and \n",
    "        static attributes, and store the in a dictionary. It will be used later to standardize used in the LSTM. This\n",
    "        function should be called only in training period. \n",
    "        \"\"\"\n",
    "        global_x = np.vstack([df.loc[:, self.dynamic_input].values for df in self.df_ts.values()])\n",
    "        self.scaler['x_d_mean'] = torch.tensor(np.nanmean(global_x, axis=0), dtype=torch.float32)\n",
    "        self.scaler['x_d_std'] = torch.tensor(np.nanstd(global_x, axis=0), dtype=torch.float32)\n",
    "        del global_x\n",
    "\n",
    "        global_y = np.vstack([df.loc[:, self.target].values for df in self.df_ts.values()])\n",
    "        self.scaler['y_mean'] = torch.tensor(np.nanmean(global_y, axis=0), dtype=torch.float32)\n",
    "        self.scaler['y_std'] = torch.tensor(np.nanstd(global_y, axis=0), dtype=torch.float32)\n",
    "        del global_y\n",
    "\n",
    "        if self.static_input:\n",
    "            self.scaler['x_s_mean'] = torch.tensor(self.df_attributes.mean().values, dtype= torch.float32)\n",
    "            self.scaler['x_s_std'] = torch.tensor(self.df_attributes.std().values, dtype= torch.float32)\n",
    "    \n",
    "    def standardize_data(self, standardize_output:bool=True):\n",
    "        \"\"\"Standardize the data used in the LSTM. \n",
    "\n",
    "        The function standardize the data contained in the self.sequence_data dictionary \n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        standardize_output : bool\n",
    "            Boolean to define if the output should be standardize or not. \n",
    "        \"\"\"\n",
    "        for basin in self.sequence_data.values():\n",
    "            # Standardize input\n",
    "            basin['x_d'] = (basin['x_d'] - self.scaler['x_d_mean']) / self.scaler['x_d_std']\n",
    "            if self.static_input:\n",
    "                basin['x_s'] = (basin['x_s'] - self.scaler['x_s_mean']) / self.scaler['x_s_std']\n",
    "            # Standardize output\n",
    "            if standardize_output:\n",
    "                basin['y'] = (basin['y'] - self.scaler['y_mean']) / self.scaler['y_std']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 3. Create the different datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset training\n",
    "training_dataset = BaseDataset(dynamic_input=dynamic_input,\n",
    "                               static_input=static_input,\n",
    "                               target=target,\n",
    "                               sequence_length=model_hyper_parameters['seq_length'],\n",
    "                               time_period=training_period,\n",
    "                               path_entities=path_entities,\n",
    "                               path_data=path_data,\n",
    "                               check_NaN=True)\n",
    "\n",
    "training_dataset.calculate_basin_std()\n",
    "training_dataset.calculate_global_statistics() # the global statistics are calculated in the training period!\n",
    "training_dataset.standardize_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset validation\n",
    "validation_dataset = BaseDataset(dynamic_input=dynamic_input,\n",
    "                                 static_input=static_input,\n",
    "                                 target=target,\n",
    "                                 sequence_length=model_hyper_parameters['seq_length'],\n",
    "                                 time_period=validation_period,\n",
    "                                 path_entities=path_entities,\n",
    "                                 path_data=path_data,\n",
    "                                 check_NaN=False)\n",
    "\n",
    "validation_dataset.scaler = training_dataset.scaler # read the global statisctics calculated in the training period\n",
    "validation_dataset.standardize_data(standardize_output=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 4. Create the different dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataLoader for training data.\n",
    "train_loader = DataLoader(training_dataset, \n",
    "                          batch_size=model_hyper_parameters['batch_size'],\n",
    "                          shuffle=True,\n",
    "                          drop_last = True)\n",
    "\n",
    "print('Batches in training: ', len(train_loader))\n",
    "x_lstm, y, per_basin_target_std = next(iter(train_loader))\n",
    "print(f'x_lstm: {x_lstm.shape} | y: {y.shape} | per_basin_target_std: {per_basin_target_std.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataLoader for validation data.\n",
    "validation_batches=[[index for index, _ in group] for _ , group in groupby(enumerate(validation_dataset.valid_entities), \n",
    "                                                                           lambda x: x[1][0])] # each basin is one batch\n",
    "\n",
    "validation_loader = DataLoader(dataset=validation_dataset,\n",
    "                               batch_sampler=validation_batches)\n",
    "\n",
    "# see if the batches are loaded correctly\n",
    "print('Batches in validation: ', len(validation_loader))\n",
    "x_lstm, y= next(iter(validation_loader))\n",
    "print(f'x_lstm: {x_lstm.shape} | y: {y.shape}')\n",
    "\n",
    "# create some lists with the valid basins and the valid entities per basin that will help later to organize the data\n",
    "valid_basins = [next(group)[0] for key, group in groupby(validation_dataset.valid_entities, key=lambda x: x[0])]\n",
    "valid_entity_per_basin = [[id for _, id in group] for key, group in groupby(validation_dataset.valid_entities, \n",
    "                                                                            key=lambda x: x[0])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 5. Define LSTM class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if model will be run in gpu or cpu\n",
    "if running_device == 'gpu':\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "    device= f'cuda:0'\n",
    "elif running_device == 'cpu':\n",
    "    device = \"cpu\"\n",
    "\n",
    "class Cuda_LSTM(nn.Module):\n",
    "    def __init__(self, model_hyper_parameters):\n",
    "        super().__init__()\n",
    "        self.num_features = model_hyper_parameters['input_size']\n",
    "        self.hidden_units = model_hyper_parameters['hidden_size']\n",
    "        self.num_layers = model_hyper_parameters['no_of_layers']\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size = model_hyper_parameters['input_size'], \n",
    "                            hidden_size = model_hyper_parameters['hidden_size'], \n",
    "                            batch_first = True,\n",
    "                            num_layers = model_hyper_parameters['no_of_layers'])\n",
    "\n",
    "        \n",
    "        self.dropout = torch.nn.Dropout(model_hyper_parameters['drop_out'])\n",
    "        self.linear = nn.Linear(in_features=model_hyper_parameters['hidden_size'], out_features=1)\n",
    "           \n",
    "    def forward(self, x):\n",
    "        # initialize hidden state with zeros\n",
    "        batch_size = x.shape[0]\n",
    "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_units, requires_grad=True, dtype=torch.float32, \n",
    "                         device=x.device)\n",
    "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_units, requires_grad=True, dtype=torch.float32,\n",
    "                         device=x.device)\n",
    "        \n",
    "        out, (hn_1, cn_1) = self.lstm(x, (h0, c0))\n",
    "        out = out[:,-1,:] # sequence to one\n",
    "        out = self.dropout(out)\n",
    "        out = self.linear(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 6. Train LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct model\n",
    "set_random_seed(seed=seed)\n",
    "lstm_model = Cuda_LSTM(model_hyper_parameters).to(device)\n",
    "\n",
    "# optimizer\n",
    "optimizer = torch.optim.Adam(lstm_model.parameters(),\n",
    "                             lr=model_hyper_parameters[\"learning_rate\"])\n",
    "    \n",
    "# define learning rate scheduler\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer,\n",
    "                                            step_size=model_hyper_parameters[\"adapt_learning_rate_epoch\"],\n",
    "                                            gamma=model_hyper_parameters[\"adapt_gamma_learning_rate\"])\n",
    "\n",
    "# set forget gate to 3 to ensure that the model is capable to learn long term dependencies\n",
    "lstm_model.lstm.bias_hh_l0.data[model_hyper_parameters['hidden_size']:2 * model_hyper_parameters['hidden_size']]=model_hyper_parameters[\"set_forget_gate\"]\n",
    "\n",
    "training_time = time.time()\n",
    "# Loop through the different epochs\n",
    "for epoch in range(model_hyper_parameters[\"no_of_epochs\"]):\n",
    "    \n",
    "    epoch_start_time = time.time()\n",
    "    total_loss = 0\n",
    "    # Training ----------------------------------------------------------------------\n",
    "    lstm_model.train()\n",
    "    for (x_lstm, y, per_basin_target_std) in train_loader: \n",
    "        optimizer.zero_grad() # sets gradients of weigths and bias to zero\n",
    "        y_sim  = lstm_model(x_lstm.to(device)) # forward call\n",
    "        \n",
    "        loss = nse_basin_averaged_loss(y_sim=y_sim, \n",
    "                                       y_obs=y.to(device), \n",
    "                                       per_basin_target_std=per_basin_target_std.to(device))\n",
    "        \n",
    "        loss.backward() # backpropagates\n",
    "        optimizer.step() #update weights\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        # remove from cuda\n",
    "        del x_lstm, y, y_sim, per_basin_target_std\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "    #average loss training   \n",
    "    average_loss_training = total_loss / len(train_loader)\n",
    "    \n",
    "    # Validation ----------------------------------------------------------------------\n",
    "    lstm_model.eval()\n",
    "    validation_results={}\n",
    "    with torch.no_grad():\n",
    "        for i, (x_lstm, y) in enumerate(validation_loader): \n",
    "            # run LSTM\n",
    "            y_sim = lstm_model(x_lstm.to(device)) # forward call\n",
    "            \n",
    "            # scale back prediction\n",
    "            y_sim = y_sim* validation_dataset.scaler['y_std'].to(device) + validation_dataset.scaler['y_mean'].to(device)\n",
    "\n",
    "            # join results in a dataframe and store them in a dictionary (is easier to plot later)\n",
    "            df_ts = validation_dataset.df_ts[valid_basins[i]].iloc[valid_entity_per_basin[i]]\n",
    "            df_new = pd.DataFrame(data={'y_obs': y.flatten().cpu().detach().numpy(), \n",
    "                                        'y_sim': y_sim.flatten().cpu().detach().numpy()}, index=df_ts.index)\n",
    "            df_ts = pd.concat([df_ts, df_new], axis=1)\n",
    "            df_ts = df_ts.filter(['y_obs', 'y_sim'])\n",
    "            validation_results[valid_basins[i]] = df_ts\n",
    "            \n",
    "            # remove from cuda\n",
    "            del x_lstm, y, y_sim\n",
    "            torch.cuda.empty_cache()       \n",
    "            \n",
    "        #average loss validation\n",
    "        loss_validation = nse_loss(df_results=validation_results)\n",
    "\n",
    "\n",
    "    # save model after every epoch\n",
    "    path_saved_model = path_save_folder+'/epoch_' + str(epoch+1)\n",
    "    torch.save(lstm_model.state_dict(), path_saved_model)\n",
    "            \n",
    "    # print epoch report\n",
    "    epoch_training_time = time.time()-epoch_start_time\n",
    "    LR = optimizer.param_groups[0]['lr']\n",
    "    report = f'Epoch: {epoch+1:<2} | Loss training: {\"%.3f \"% (average_loss_training)} | NSE validation: {\"%.3f \"% (loss_validation)} | LR:{\"%.5f \"% (LR)} | Training time: {\"%.1f \"% (epoch_training_time)} s'\n",
    "    print(report)\n",
    "    # save epoch report in txt file\n",
    "    write_report(file_path=path_save_folder+'/run_progress.txt', text=report)\n",
    "    # modify learning rate\n",
    "    scheduler.step()\n",
    "\n",
    "# print total report\n",
    "total_training_time = time.time()-training_time\n",
    "report = f'Total training time: {\"%.1f \"% (total_training_time)} s'\n",
    "print(report)\n",
    "# save total report in txt file\n",
    "write_report(file_path=path_save_folder+'/run_progress.txt', text=report)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 7. Test LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In case I already trained an LSTM I can re-construct the model\n",
    "lstm_model = Cuda_LSTM(model_hyper_parameters).to(device)\n",
    "lstm_model.load_state_dict(torch.load(path_save_folder + '/epoch_20'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset testing\n",
    "path_entities = '../data/basin_id/basins_camels_gb_lees.txt'\n",
    "test_dataset = BaseDataset(dynamic_input=dynamic_input,\n",
    "                           static_input=static_input,\n",
    "                           target=target,\n",
    "                           sequence_length=model_hyper_parameters['seq_length'],\n",
    "                           time_period=testing_period,\n",
    "                           path_entities=path_entities,\n",
    "                           path_data=path_data,\n",
    "                           check_NaN=False)\n",
    "\n",
    "test_dataset.scaler = training_dataset.scaler # read the global statisctics calculated in the training period\n",
    "test_dataset.standardize_data(standardize_output=False)\n",
    "\n",
    "# DataLoader for testing data.\n",
    "test_batches=[[index for index, _ in group] for _ , group in groupby(enumerate(test_dataset.valid_entities), \n",
    "                                                                     lambda x: x[1][0])]\n",
    "\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_sampler=test_batches)\n",
    "\n",
    "# see if the batches are loaded correctly\n",
    "print('Batches in testing: ', len(test_loader))\n",
    "x_lstm, y= next(iter(test_loader))\n",
    "print(f'x_lstm: {x_lstm.shape} | y: {y.shape}')\n",
    "\n",
    "# create some lists with the valid basins and the valid entities per basin that will help later to organize the data\n",
    "valid_basins_testing = [next(group)[0] for key, group in groupby(test_dataset.valid_entities, key=lambda x: x[0])]\n",
    "valid_entity_per_basin_testing = [[id for _, id in group] for key, group in groupby(test_dataset.valid_entities, \n",
    "                                                                                    key=lambda x: x[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_model.eval()\n",
    "test_results = {}\n",
    "# Testing----------------------------------------------------------------------\n",
    "with torch.no_grad():\n",
    "    for i, (x_lstm, y) in enumerate(test_loader):\n",
    "        # run LSTM\n",
    "        y_sim = lstm_model(x_lstm.to(device)) # forward call\n",
    "        \n",
    "        # scale back prediction\n",
    "        y_sim = y_sim* test_dataset.scaler['y_std'].to(device) + test_dataset.scaler['y_mean'].to(device)\n",
    "\n",
    "        # join results in a dataframe and store them in a dictionary (is easier to plot later)\n",
    "        df_ts = test_dataset.df_ts[valid_basins_testing[i]].iloc[valid_entity_per_basin_testing[i]]\n",
    "        df_new = pd.DataFrame(data={'y_obs': y.flatten().cpu().detach().numpy(), \n",
    "                                    'y_sim': y_sim.flatten().cpu().detach().numpy()}, index=df_ts.index)\n",
    "        df_ts = pd.concat([df_ts, df_new], axis=1)\n",
    "        df_ts = df_ts.filter(['y_obs', 'y_sim'])\n",
    "        test_results[valid_basins_testing[i]] = df_ts\n",
    "        \n",
    "        # remove from cuda\n",
    "        del x_lstm, y, y_sim\n",
    "        torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 8. Initial analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss testing\n",
    "loss_testing = nse_loss(df_results=test_results, average=False)\n",
    "df_NSE = pd.DataFrame(data={'basin_id': valid_basins_testing, 'NSE': np.round(loss_testing,3)})\n",
    "df_NSE = df_NSE.set_index('basin_id')\n",
    "#df_NSE.to_csv(path_save_folder+'/NSE.csv', index=True, header=True)\n",
    "\n",
    "# Simulated and observed results\n",
    "y_sim = pd.concat([pd.DataFrame({key: value['y_sim']}) for key, value in test_results.items()], axis=1)\n",
    "y_obs = pd.concat([pd.DataFrame({key: value['y_obs']}) for key, value in test_results.items()], axis=1)\n",
    "\n",
    "# Set index same as last dataframe in loop (because of how the dataframes where constructed all have the same indexes)\n",
    "y_sim = y_sim.set_index(list(test_results.values())[0]['y_sim'].index)\n",
    "y_obs = y_obs.set_index(list(test_results.values())[0]['y_obs'].index)\n",
    "\n",
    "# Export the results\n",
    "#y_sim.to_csv(path_save_folder+'/y_sim.csv', index=True, header=True)\n",
    "#y_obs.to_csv(path_save_folder+'/y_obs.csv', index=True, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Plot the histogram\n",
    "plt.hist(df_NSE['NSE'], bins=[0, .1, .2, .3, .4, .5, .6, .7, .8, .9, 1])\n",
    "\n",
    "# Add NSE statistics to the plot\n",
    "plt.text(0.01, 0.8, f'Mean: {\"%.2f\" % df_NSE[\"NSE\"].mean():>7}\\nMedian: {\"%.2f\" % df_NSE[\"NSE\"].median():>0}\\nMax: {\"%.2f\" % df_NSE[\"NSE\"].max():>9}\\nMin: {\"%.2f\" % df_NSE[\"NSE\"].min():>10}',\n",
    "         transform=plt.gca().transAxes, bbox=dict(facecolor='white', alpha=0.5))\n",
    "\n",
    "# Format plot\n",
    "plt.rcParams[\"figure.figsize\"] = (20, 5)\n",
    "plt.xlabel('NSE', fontsize=12, fontweight='bold')\n",
    "plt.ylabel('Frequency', fontsize=12, fontweight='bold')\n",
    "plt.title('NSE Histogram', fontsize=16, fontweight='bold')\n",
    "#plt.savefig(save_folder+'/NSE_LSTM_Histogram.png', bbox_inches='tight', pad_inches=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot simulated and observed discharges\n",
    "basin_to_analyze = '54008'\n",
    "\n",
    "plt.plot(y_obs[basin_to_analyze], label=\"observed\", color=color_palette[\"observed\"])\n",
    "plt.plot(y_sim[basin_to_analyze], label=\"simulated\", alpha=0.5, color=color_palette[\"LSTM\"])\n",
    "\n",
    "# Format plot\n",
    "plt.xlabel('Day', fontsize=12, fontweight='bold')\n",
    "plt.ylabel('Discharge [mm/d]', fontsize=12, fontweight='bold')\n",
    "plt.title('Discharge comparison', fontsize=16, fontweight='bold')\n",
    "plt.tick_params(axis='both', which='major', labelsize=12)\n",
    "plt.legend(loc=\"upper right\",fontsize=12)\n",
    "#plt.savefig(save_folder+'/Model_Comparison.png', bbox_inches='tight', pad_inches=0)"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "pytorch-gpu.2-0.m111",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.2-0:m111"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "42b7dc197ee81dd2f6541889b0e14556b882d218c1e7c97db94bc0f7b191f034"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
